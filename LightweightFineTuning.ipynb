{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project implements state-of-the-art Parameter-Efficient Fine-Tuning (PEFT) techniques to adapt pre-trained language models for sentiment analysis with minimal computational resources. We explore both LoRA (Low-Rank Adaptation) and QLoRA (Quantized LoRA) approaches, along with extensive analysis of different configurations.\n",
    "\n",
    "## Project Choices\n",
    "\n",
    "* **PEFT techniques:** \n",
    "  * LoRA (Low-Rank Adaptation) with multiple configuration experiments\n",
    "  * QLoRA (Quantized LoRA) for additional memory efficiency\n",
    "\n",
    "* **Model:** \n",
    "  * DistilBERT base model (not pre-fine-tuned) for sentiment analysis\n",
    "  * Selection provides good balance between model capacity and inference efficiency\n",
    "\n",
    "* **Evaluation approach:** \n",
    "  * Comprehensive metrics suite (accuracy, F1, precision, recall)\n",
    "  * Statistical significance testing\n",
    "  * Detailed visualizations for performance analysis\n",
    "  * Memory profiling for efficiency analysis\n",
    "\n",
    "* **Fine-tuning dataset:** \n",
    "  * GLUE SST-2 (Stanford Sentiment Treebank) - binary sentiment classification\n",
    "  * Well-established benchmark for sentiment analysis tasks\n",
    "\n",
    "* **Enhancements:** \n",
    "  * Multiple LoRA configuration comparisons\n",
    "  * QLoRA implementation with memory optimization\n",
    "  * Real-world inference examples and applications\n",
    "  * Detailed performance visualizations\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "1. Setup and Initialization\n",
    "2. Dataset Loading and Preprocessing\n",
    "3. Base Model Evaluation\n",
    "4. LoRA Implementation and Training\n",
    "5. QLoRA Implementation and Training\n",
    "6. LoRA Configuration Experimentation\n",
    "7. Model Comparison and Analysis\n",
    "8. Real-World Applications\n",
    "9. Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b6b07e",
   "metadata": {},
   "source": [
    "## Setup and imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9302140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: scikit-learn in /home/student/.local/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /home/student/.local/lib/python3.10/site-packages (3.8.3)\n",
      "Requirement already satisfied: seaborn in /home/student/.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: pandas in /home/student/.local/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy in /home/student/.local/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/student/.local/lib/python3.10/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/student/.local/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/student/.local/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/student/.local/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/student/.local/lib/python3.10/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: requests in /home/student/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/student/.local/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/student/.local/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/student/.local/lib/python3.10/site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/student/.local/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/student/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pillow>=8 in /home/student/.local/lib/python3.10/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/student/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/student/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/student/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Using device: cuda\n",
      "GPU: Tesla T4\n",
      "CUDA Version: 11.7\n",
      "PyTorch Version: 2.0.1\n",
      "Total GPU Memory: 15.64 GB\n",
      "Current Memory Usage: 4.59 GB\n",
      "\n",
      "Project configuration:\n",
      "  model_name: distilbert-base-uncased\n",
      "  max_length: 128\n",
      "  train_sample_size: 5000\n",
      "  validation_sample_size: 500\n",
      "  lora_r: 16\n",
      "  lora_alpha: 32\n",
      "  lora_dropout: 0.1\n",
      "  batch_size: 16\n",
      "  learning_rate: 0.0005\n",
      "  epochs: 3\n",
      "  weight_decay: 0.01\n",
      "  save_steps: 500\n",
      "  qlora_batch_size: 8\n",
      "  qlora_learning_rate: 0.0001\n",
      "  eval_batch_size: 32\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers datasets peft scikit-learn matplotlib seaborn pandas numpy\n",
    "\n",
    "\n",
    "# Initial setup and imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# Suppress non-critical warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Hugging Face imports\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "# PEFT imports\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType, \n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "# Metrics and evaluation imports\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    f1_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    confusion_matrix, \n",
    "    classification_report,\n",
    "    roc_curve, \n",
    "    auc\n",
    ")\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Avoid parallelism warning\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Create directories for outputs\n",
    "output_dir = \"./peft_output\"\n",
    "viz_dir = f\"{output_dir}/visualizations\"\n",
    "models_dir = f\"{output_dir}/models\"\n",
    "results_dir = f\"{output_dir}/results\"\n",
    "\n",
    "# Create all required directories\n",
    "for dir_path in [output_dir, viz_dir, models_dir, results_dir]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Configure plots for consistency\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Display GPU info if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    # Memory info\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"Total GPU Memory: {gpu_props.total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Current Memory Usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Configure project parameters (these can be adjusted as needed)\n",
    "config = {\n",
    "    \"model_name\": \"distilbert-base-uncased\",  # Base model (not fine-tuned)\n",
    "    \"max_length\": 128,                        # Maximum sequence length\n",
    "    \"train_sample_size\": 5000,                # Number of training examples to use\n",
    "    \"validation_sample_size\": 500,            # Number of validation examples for quick evaluation\n",
    "    \"lora_r\": 16,                             # LoRA rank parameter\n",
    "    \"lora_alpha\": 32,                         # LoRA alpha parameter\n",
    "    \"lora_dropout\": 0.1,                      # LoRA dropout\n",
    "    \"batch_size\": 16,                         # Batch size for training\n",
    "    \"learning_rate\": 5e-4,                    # Learning rate\n",
    "    \"epochs\": 3,                              # Number of training epochs\n",
    "    \"weight_decay\": 0.01,                     # Weight decay for regularization\n",
    "    \"save_steps\": 500,                        # Save checkpoints every X steps\n",
    "    \"qlora_batch_size\": 8,                    # Smaller batch size for QLoRA\n",
    "    \"qlora_learning_rate\": 1e-4,              # Lower learning rate for QLoRA\n",
    "    \"eval_batch_size\": 32                     # Batch size for evaluation\n",
    "}\n",
    "\n",
    "print(\"\\nProject configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Dictionary to store all results for comparison\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12fdcd6",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6644470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define comprehensive helper functions for the project\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute a comprehensive set of evaluation metrics for classification.\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: Tuple of (predictions, labels) from the model\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of metrics including accuracy, F1, precision, and recall\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Core metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "        \"precision\": precision_score(labels, predictions, average=\"weighted\"),\n",
    "        \"recall\": recall_score(labels, predictions, average=\"weighted\")\n",
    "    }\n",
    "    \n",
    "    # Per-class metrics if multiple classes are present\n",
    "    unique_labels = np.unique(labels)\n",
    "    if len(unique_labels) > 1:\n",
    "        for label in unique_labels:\n",
    "            label_predictions = (predictions == label)\n",
    "            label_true = (labels == label)\n",
    "            metrics[f\"precision_class_{label}\"] = precision_score(label_true, label_predictions, zero_division=0)\n",
    "            metrics[f\"recall_class_{label}\"] = recall_score(label_true, label_predictions, zero_division=0)\n",
    "            metrics[f\"f1_class_{label}\"] = f1_score(label_true, label_predictions, zero_division=0)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Format time duration in a human-readable format.\"\"\"\n",
    "    minutes, seconds = divmod(seconds, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    return f\"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\"\n",
    "\n",
    "\n",
    "def visualize_confusion_matrix(predictions, true_labels, output_path=None, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Create and visualize a confusion matrix for the given predictions.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions (class indices)\n",
    "        true_labels: True class labels\n",
    "        output_path: Path to save the visualization (optional)\n",
    "        model_name: Name of the model for the title\n",
    "        \n",
    "    Returns:\n",
    "        The confusion matrix\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # If labels are binary, add class names\n",
    "    if cm.shape[0] == 2:\n",
    "        plt.xticks([0.5, 1.5], ['Negative', 'Positive'])\n",
    "        plt.yticks([0.5, 1.5], ['Negative', 'Positive'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if output path is specified\n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "\n",
    "def plot_metric_comparison(metrics_dict, metric_name='accuracy', title=None, output_path=None):\n",
    "    \"\"\"\n",
    "    Create a bar chart comparing a specific metric across different models.\n",
    "    \n",
    "    Args:\n",
    "        metrics_dict: Dictionary mapping model names to metric values\n",
    "        metric_name: The name of the metric to visualize\n",
    "        title: Plot title (default derived from metric name)\n",
    "        output_path: Path to save the visualization (optional)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    models = list(metrics_dict.keys())\n",
    "    values = [metrics_dict[model][f'eval_{metric_name}'] for model in models]\n",
    "    \n",
    "    # Create bar chart with custom colors\n",
    "    colors = sns.color_palette(\"muted\", len(models))\n",
    "    bars = plt.bar(models, values, color=colors)\n",
    "    \n",
    "    # Set title and labels\n",
    "    if title is None:\n",
    "        title = f'{metric_name.capitalize()} Comparison Across Models'\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel(metric_name.capitalize(), fontsize=12)\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                 f'{height:.4f}',\n",
    "                 ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    # Add a horizontal line for base model reference\n",
    "    if 'Base Model' in metrics_dict:\n",
    "        base_value = metrics_dict['Base Model'][f'eval_{metric_name}']\n",
    "        plt.axhline(y=base_value, color='red', linestyle='--', alpha=0.7, \n",
    "                    label=f'Base Model: {base_value:.4f}')\n",
    "        plt.legend()\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if output path is specified\n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved {metric_name} comparison chart to {output_path}\")\n",
    "\n",
    "\n",
    "def plot_training_history(history, output_path=None):\n",
    "    \"\"\"\n",
    "    Plot training loss and evaluation metrics from training history.\n",
    "    \n",
    "    Args:\n",
    "        history: Training history from Trainer.state.log_history\n",
    "        output_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    # Extract training loss\n",
    "    train_loss = []\n",
    "    train_steps = []\n",
    "    eval_loss = []\n",
    "    eval_acc = []\n",
    "    eval_steps = []\n",
    "    \n",
    "    for entry in history:\n",
    "        if 'loss' in entry and 'eval_loss' not in entry:\n",
    "            train_loss.append(entry['loss'])\n",
    "            train_steps.append(entry['step'])\n",
    "        if 'eval_loss' in entry:\n",
    "            eval_loss.append(entry['eval_loss'])\n",
    "            eval_acc.append(entry['eval_accuracy'])\n",
    "            eval_steps.append(entry['step'])\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_steps, train_loss, 'b-', marker='o', markersize=4, alpha=0.7)\n",
    "    plt.title('Training Loss', fontsize=14)\n",
    "    plt.xlabel('Steps', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot evaluation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(eval_steps, eval_acc, 'g-', marker='o', markersize=6, label='Accuracy')\n",
    "    \n",
    "    # If eval loss exists, plot it on secondary axis\n",
    "    if eval_loss:\n",
    "        ax2 = plt.gca().twinx()\n",
    "        ax2.plot(eval_steps, eval_loss, 'r--', marker='x', markersize=4, alpha=0.7, label='Loss')\n",
    "        ax2.set_ylabel('Loss', color='r', fontsize=12)\n",
    "        ax2.tick_params(axis='y', colors='r')\n",
    "    \n",
    "    plt.title('Validation Metrics', fontsize=14)\n",
    "    plt.xlabel('Steps', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if output path is specified\n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "        print(f\"Training history plot saved to {output_path}\")\n",
    "\n",
    "\n",
    "def plot_roc_curve(model, dataset, tokenizer, device, output_path=None, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Plot ROC curve for the model on a given dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        dataset: Evaluation dataset\n",
    "        tokenizer: Tokenizer for the model\n",
    "        device: Device to run inference on\n",
    "        output_path: Path to save the visualization\n",
    "        model_name: Name of the model for the title\n",
    "    \"\"\"\n",
    "    # Create a dataloader for the dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=16)\n",
    "    \n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].numpy()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get probabilities for positive class\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "            \n",
    "            all_labels.extend(labels)\n",
    "            all_probs.extend(probs)\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "        print(f\"ROC curve saved to {output_path}\")\n",
    "    \n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "def plot_parameter_efficiency(total_params, trainable_params, output_path=None):\n",
    "    \"\"\"\n",
    "    Create a visualization of parameter efficiency for a PEFT model.\n",
    "    \n",
    "    Args:\n",
    "        total_params: Total number of parameters in the model\n",
    "        trainable_params: Number of trainable parameters\n",
    "        output_path: Path to save the visualization\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Calculate frozen parameters\n",
    "    frozen_params = total_params - trainable_params\n",
    "    \n",
    "    # Create pie chart\n",
    "    sizes = [trainable_params, frozen_params]\n",
    "    labels = ['Trainable Parameters', 'Frozen Parameters']\n",
    "    colors = ['#ff9999', '#66b3ff']\n",
    "    explode = (0.1, 0)  # Explode the trainable parameters\n",
    "    \n",
    "    plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "            autopct='%1.1f%%', shadow=True, startangle=90,\n",
    "            textprops={'fontsize': 14})\n",
    "    \n",
    "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    plt.title(f'Parameter Efficiency: {trainable_params:,} out of {total_params:,} parameters',\n",
    "              fontsize=16)\n",
    "    \n",
    "    # Add text annotation with exact numbers\n",
    "    plt.annotate(f\"Trainable: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\\nFrozen: {frozen_params:,} ({frozen_params/total_params*100:.2f}%)\",\n",
    "                xy=(0.5, 0.05), xycoords='figure fraction',\n",
    "                horizontalalignment='center', verticalalignment='bottom',\n",
    "                fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "        print(f\"Parameter efficiency visualization saved to {output_path}\")\n",
    "\n",
    "\n",
    "def load_sst2_dataset():\n",
    "    \"\"\"\n",
    "    Load the SST-2 dataset with multiple fallback methods in case of errors.\n",
    "    Returns the dataset or raises an error if all methods fail.\n",
    "    \"\"\"\n",
    "    # Method 1: Standard loading\n",
    "    try:\n",
    "        print(\"Attempting to load SST-2 dataset directly...\")\n",
    "        dataset = load_dataset(\"glue\", \"sst2\")\n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error in standard loading: {e}\")\n",
    "    \n",
    "    # Method 2: Try with dataset builder\n",
    "    try:\n",
    "        print(\"Attempting to load with dataset builder...\")\n",
    "        from datasets import load_dataset_builder\n",
    "        builder = load_dataset_builder(\"glue\", \"sst2\")\n",
    "        builder.download_and_prepare()\n",
    "        dataset = builder.as_dataset()\n",
    "        print(\"Dataset loaded successfully with builder!\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error with dataset builder: {e}\")\n",
    "    \n",
    "    # Method 3: Manual download\n",
    "    try:\n",
    "        print(\"Attempting direct download of SST-2...\")\n",
    "        # Download raw data if not already present\n",
    "        if not os.path.exists('SST-2.zip'):\n",
    "            !wget -q https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
    "            !unzip -q SST-2.zip\n",
    "        elif not os.path.exists('SST-2'):\n",
    "            !unzip -q SST-2.zip\n",
    "        \n",
    "        # Load train and dev data\n",
    "        train_df = pd.read_csv('SST-2/train.tsv', sep='\\t')\n",
    "        dev_df = pd.read_csv('SST-2/dev.tsv', sep='\\t')\n",
    "        \n",
    "        # Rename columns to match expected format if needed\n",
    "        if 'sentence' not in train_df.columns and 'text' in train_df.columns:\n",
    "            train_df = train_df.rename(columns={'text': 'sentence'})\n",
    "            dev_df = dev_df.rename(columns={'text': 'sentence'})\n",
    "        \n",
    "        # Convert to datasets format\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        validation_dataset = Dataset.from_pandas(dev_df)\n",
    "        \n",
    "        # Create dataset dictionary\n",
    "        dataset = DatasetDict({\n",
    "            'train': train_dataset,\n",
    "            'validation': validation_dataset\n",
    "        })\n",
    "        print(\"Dataset loaded successfully via direct download!\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error with direct download: {e}\")\n",
    "        raise ValueError(\"All dataset loading methods failed. Please check your environment.\")\n",
    "\n",
    "\n",
    "def format_for_pytorch(dataset):\n",
    "    \"\"\"\n",
    "    Format a dataset for PyTorch by removing unnecessary columns \n",
    "    and renaming labels.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Hugging Face dataset\n",
    "        \n",
    "    Returns:\n",
    "        Formatted dataset\n",
    "    \"\"\"\n",
    "    # Remove unnecessary columns\n",
    "    if \"sentence\" in dataset.column_names:\n",
    "        dataset = dataset.remove_columns([\"sentence\"])\n",
    "    if \"idx\" in dataset.column_names:\n",
    "        dataset = dataset.remove_columns([\"idx\"])\n",
    "    \n",
    "    # Rename label to labels for Trainer compatibility\n",
    "    if \"label\" in dataset.column_names and \"labels\" not in dataset.column_names:\n",
    "        dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "    \n",
    "    # Set format to PyTorch tensors\n",
    "    dataset.set_format(\"torch\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def track_memory_usage(model, device):\n",
    "    \"\"\"Track memory usage of a model on a specific device.\"\"\"\n",
    "    if device.type == \"cuda\":\n",
    "        memory_allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        memory_reserved = torch.cuda.memory_reserved(device) / (1024 ** 2)\n",
    "        return {\n",
    "            \"allocated_mb\": memory_allocated,\n",
    "            \"reserved_mb\": memory_reserved\n",
    "        }\n",
    "    else:\n",
    "        # For CPU, use approximate size based on parameters\n",
    "        param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "        buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "        total_size = (param_size + buffer_size) / (1024 ** 2)  # Convert to MB\n",
    "        return {\n",
    "            \"allocated_mb\": total_size,\n",
    "            \"reserved_mb\": total_size\n",
    "        }\n",
    "\n",
    "\n",
    "def get_model_size_estimate(model, as_string=False):\n",
    "    \"\"\"Get an estimate of model size in MB.\"\"\"\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    total_size_mb = (param_size + buffer_size) / (1024 ** 2)  # Convert to MB\n",
    "    \n",
    "    if as_string:\n",
    "        if total_size_mb > 1024:\n",
    "            return f\"{total_size_mb/1024:.2f} GB\"\n",
    "        else:\n",
    "            return f\"{total_size_mb:.2f} MB\"\n",
    "    else:\n",
    "        return total_size_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87559735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST-2 dataset...\n",
      "Attempting to load SST-2 dataset directly...\n",
      "Dataset loaded successfully!\n",
      "\n",
      "Dataset Statistics:\n",
      "Training examples: 67349\n",
      "Validation examples: 872\n",
      "Example data point: {'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0}\n",
      "\n",
      "Using 5000 training examples (subset of full dataset)\n",
      "Using 500 validation examples for quick evaluations\n",
      "\n",
      "Class distribution in training set:\n",
      "  Label 1: 37569 examples (55.8%)\n",
      "  Label 0: 29780 examples (44.2%)\n",
      "Class distribution visualization saved to ./peft_output/visualizations/class_distribution.png\n",
      "\n",
      "Loading tokenizer...\n",
      "Tokenizer loaded with vocabulary size: 30522\n",
      "\n",
      "Preprocessing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b53797dd7f4e7eab201121bc8c1dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation subset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing datasets for PyTorch...\n",
      "Preprocessing completed in 0.17 seconds\n",
      "\n",
      "Sample processed example:\n",
      "Input shape: torch.Size([128])\n",
      "Attention mask shape: torch.Size([128])\n",
      "Label: 1\n",
      "\n",
      "Decoded text: klein, charming in comedies like american pie and dead - on in election,\n",
      "\n",
      "Dataset preprocessing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare the SST-2 dataset\n",
    "print(\"Loading SST-2 dataset...\")\n",
    "try:\n",
    "    dataset = load_sst2_dataset()\n",
    "    \n",
    "    # Dataset statistics\n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"Training examples: {len(dataset['train'])}\")\n",
    "    print(f\"Validation examples: {len(dataset['validation'])}\")\n",
    "    print(f\"Example data point: {dataset['train'][0]}\")\n",
    "    \n",
    "    # Create smaller subsets for training and quick evaluation\n",
    "    train_dataset = dataset['train'].shuffle(seed=SEED).select(range(config['train_sample_size']))\n",
    "    validation_subset = dataset['validation'].shuffle(seed=SEED).select(range(config['validation_sample_size']))\n",
    "    \n",
    "    print(f\"\\nUsing {len(train_dataset)} training examples (subset of full dataset)\")\n",
    "    print(f\"Using {len(validation_subset)} validation examples for quick evaluations\")\n",
    "    \n",
    "    # Check class balance\n",
    "    if 'label' in dataset['train'].features:\n",
    "        labels = dataset['train']['label']\n",
    "        label_counts = pd.Series(labels).value_counts()\n",
    "        \n",
    "        print(\"\\nClass distribution in training set:\")\n",
    "        for label, count in label_counts.items():\n",
    "            print(f\"  Label {label}: {count} examples ({count/len(labels)*100:.1f}%)\")\n",
    "        \n",
    "        # Visualize class distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Create a prettier bar chart\n",
    "        ax = sns.barplot(x=label_counts.index.astype(str), y=label_counts.values, palette='viridis')\n",
    "        \n",
    "        # Add count and percentage labels\n",
    "        for i, (label, count) in enumerate(label_counts.items()):\n",
    "            ax.text(i, count/2, f\"{count}\\n({count/len(labels)*100:.1f}%)\", \n",
    "                   ha='center', va='center', color='white', fontweight='bold')\n",
    "        \n",
    "        # Customize the plot\n",
    "        if len(label_counts) == 2:\n",
    "            plt.xticks([0, 1], ['Negative (0)', 'Positive (1)'])\n",
    "        \n",
    "        plt.title('Class Distribution in Training Set', fontsize=14)\n",
    "        plt.xlabel('Class Label', fontsize=12)\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        plt.savefig(f\"{viz_dir}/class_distribution.png\")\n",
    "        plt.close()\n",
    "        print(f\"Class distribution visualization saved to {viz_dir}/class_distribution.png\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"\\nLoading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
    "    print(f\"Tokenizer loaded with vocabulary size: {len(tokenizer)}\")\n",
    "    \n",
    "    # Define preprocessing function\n",
    "    def preprocess_function(examples):\n",
    "        \"\"\"Tokenize and prepare examples for the model.\"\"\"\n",
    "        return tokenizer(\n",
    "            examples[\"sentence\"] if \"sentence\" in examples else examples[\"text\"], \n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=config['max_length']\n",
    "        )\n",
    "    \n",
    "    # Process the datasets\n",
    "    print(\"\\nPreprocessing datasets...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    encoded_train = train_dataset.map(\n",
    "        preprocess_function, \n",
    "        batched=True, \n",
    "        desc=\"Tokenizing training set\"\n",
    "    )\n",
    "    \n",
    "    encoded_validation = dataset['validation'].map(\n",
    "        preprocess_function, \n",
    "        batched=True,\n",
    "        desc=\"Tokenizing validation set\"\n",
    "    )\n",
    "    \n",
    "    encoded_validation_subset = validation_subset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        desc=\"Tokenizing validation subset\"\n",
    "    )\n",
    "    \n",
    "    # Prepare datasets for PyTorch\n",
    "    print(\"\\nPreparing datasets for PyTorch...\")\n",
    "    encoded_train = format_for_pytorch(encoded_train)\n",
    "    encoded_validation = format_for_pytorch(encoded_validation)\n",
    "    encoded_validation_subset = format_for_pytorch(encoded_validation_subset)\n",
    "    \n",
    "    preprocess_time = time.time() - start_time\n",
    "    print(f\"Preprocessing completed in {preprocess_time:.2f} seconds\")\n",
    "    \n",
    "    # Examine one processed example to verify\n",
    "    print(\"\\nSample processed example:\")\n",
    "    example_idx = 0\n",
    "    sample_input_ids = encoded_train[example_idx]['input_ids']\n",
    "    sample_attention_mask = encoded_train[example_idx]['attention_mask']\n",
    "    sample_label = encoded_train[example_idx]['labels']\n",
    "    \n",
    "    print(f\"Input shape: {sample_input_ids.shape}\")\n",
    "    print(f\"Attention mask shape: {sample_attention_mask.shape}\")\n",
    "    print(f\"Label: {sample_label}\")\n",
    "    \n",
    "    # Decode a processed example to verify tokenization\n",
    "    decoded_text = tokenizer.decode(sample_input_ids, skip_special_tokens=True)\n",
    "    print(f\"\\nDecoded text: {decoded_text}\")\n",
    "    \n",
    "    print(\"\\nDataset preprocessing completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error during dataset preparation: {e}\")\n",
    "    traceback.print_exc()\n",
    "    raise RuntimeError(\"Dataset preparation failed. Cannot proceed with the project.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "418066ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: distilbert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture Summary:\n",
      "Model type: distilbert\n",
      "Number of parameters: 66,955,010\n",
      "Estimated model size: 255.42 MB\n",
      "\n",
      "Potential target modules for LoRA:\n",
      "  distilbert.transformer.layer.0.attention\n",
      "  distilbert.transformer.layer.0.attention.dropout\n",
      "  distilbert.transformer.layer.0.attention.q_lin\n",
      "  distilbert.transformer.layer.0.attention.k_lin\n",
      "  distilbert.transformer.layer.0.attention.v_lin\n",
      "  distilbert.transformer.layer.0.attention.out_lin\n",
      "  distilbert.transformer.layer.1.attention\n",
      "  distilbert.transformer.layer.1.attention.dropout\n",
      "  distilbert.transformer.layer.1.attention.q_lin\n",
      "  distilbert.transformer.layer.1.attention.k_lin\n",
      "  distilbert.transformer.layer.1.attention.v_lin\n",
      "  distilbert.transformer.layer.1.attention.out_lin\n",
      "  distilbert.transformer.layer.2.attention\n",
      "  distilbert.transformer.layer.2.attention.dropout\n",
      "  distilbert.transformer.layer.2.attention.q_lin\n",
      "  distilbert.transformer.layer.2.attention.k_lin\n",
      "  distilbert.transformer.layer.2.attention.v_lin\n",
      "  distilbert.transformer.layer.2.attention.out_lin\n",
      "  distilbert.transformer.layer.3.attention\n",
      "  distilbert.transformer.layer.3.attention.dropout\n",
      "  distilbert.transformer.layer.3.attention.q_lin\n",
      "  distilbert.transformer.layer.3.attention.k_lin\n",
      "  distilbert.transformer.layer.3.attention.v_lin\n",
      "  distilbert.transformer.layer.3.attention.out_lin\n",
      "  distilbert.transformer.layer.4.attention\n",
      "  distilbert.transformer.layer.4.attention.dropout\n",
      "  distilbert.transformer.layer.4.attention.q_lin\n",
      "  distilbert.transformer.layer.4.attention.k_lin\n",
      "  distilbert.transformer.layer.4.attention.v_lin\n",
      "  distilbert.transformer.layer.4.attention.out_lin\n",
      "  distilbert.transformer.layer.5.attention\n",
      "  distilbert.transformer.layer.5.attention.dropout\n",
      "  distilbert.transformer.layer.5.attention.q_lin\n",
      "  distilbert.transformer.layer.5.attention.k_lin\n",
      "  distilbert.transformer.layer.5.attention.v_lin\n",
      "  distilbert.transformer.layer.5.attention.out_lin\n",
      "\n",
      "Creating evaluation trainer for the base model...\n",
      "\n",
      "Evaluating the foundation model (this may take a few minutes)...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base model evaluation completed in 0.98 seconds\n",
      "\n",
      "Base model results:\n",
      "--------------------------------------------------\n",
      "eval_loss                : 0.6972\n",
      "eval_accuracy            : 0.4346\n",
      "eval_f1                  : 0.3621\n",
      "eval_precision           : 0.3897\n",
      "eval_recall              : 0.4346\n",
      "eval_precision_class_0   : 0.4555\n",
      "eval_recall_class_0      : 0.7780\n",
      "eval_f1_class_0          : 0.5746\n",
      "eval_precision_class_1   : 0.3262\n",
      "eval_recall_class_1      : 0.1036\n",
      "eval_f1_class_1          : 0.1573\n",
      "eval_steps_per_second    : 28.7640\n",
      "\n",
      "Generating predictions for analysis...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.46      0.78      0.57       428\n",
      "    Positive       0.33      0.10      0.16       444\n",
      "\n",
      "    accuracy                           0.43       872\n",
      "   macro avg       0.39      0.44      0.37       872\n",
      "weighted avg       0.39      0.43      0.36       872\n",
      "\n",
      "ROC curve saved to ./peft_output/visualizations/base_model_roc_curve.png\n",
      "\n",
      "Visualizing prediction confidence distribution...\n",
      "\n",
      "Base model evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Load and evaluate the base model\n",
    "\n",
    "print(f\"Loading base model: {config['model_name']}...\")\n",
    "try:\n",
    "    # Load the base model for sequence classification\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config['model_name'],\n",
    "        num_labels=2  # Binary classification\n",
    "    )\n",
    "    base_model = base_model.to(device)\n",
    "    \n",
    "    # Record memory usage before training\n",
    "    base_memory = track_memory_usage(base_model, device)\n",
    "    \n",
    "    # Model architecture summary\n",
    "    print(f\"\\nModel Architecture Summary:\")\n",
    "    print(f\"Model type: {base_model.config.model_type}\")\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in base_model.parameters()):,}\")\n",
    "    print(f\"Estimated model size: {get_model_size_estimate(base_model, as_string=True)}\")\n",
    "    \n",
    "    # List potential target modules for LoRA\n",
    "    print(\"\\nPotential target modules for LoRA:\")\n",
    "    target_modules = []\n",
    "    \n",
    "    for name, module in base_model.named_modules():\n",
    "        if any(key in name for key in ['query', 'key', 'value', 'q_lin', 'k_lin', 'v_lin', 'out_lin', 'attention']):\n",
    "            target_modules.append(name)\n",
    "            print(f\"  {name}\")\n",
    "    \n",
    "    # Set up evaluation arguments\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=f\"{output_dir}/base_eval\",\n",
    "        per_device_eval_batch_size=config['eval_batch_size'],\n",
    "        do_train=False,\n",
    "        do_eval=True,\n",
    "        report_to=\"none\"  # Disable wandb\n",
    "    )\n",
    "    \n",
    "    # Create a Trainer for the base model\n",
    "    print(\"\\nCreating evaluation trainer for the base model...\")\n",
    "    base_trainer = Trainer(\n",
    "        model=base_model,\n",
    "        args=eval_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        eval_dataset=encoded_validation,\n",
    "    )\n",
    "    \n",
    "    # Evaluate the base model with timing\n",
    "    print(\"\\nEvaluating the foundation model (this may take a few minutes)...\")\n",
    "    start_time = time.time()\n",
    "    base_model_results = base_trainer.evaluate()\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    # Print evaluation results with formatted output\n",
    "    print(f\"\\nBase model evaluation completed in {eval_time:.2f} seconds\")\n",
    "    print(\"\\nBase model results:\")\n",
    "    print(\"-\" * 50)\n",
    "    for metric, value in base_model_results.items():\n",
    "        # Skip metrics that are not relevant for display\n",
    "        if not metric.startswith('eval_runtime') and not metric.startswith('eval_samples_per'):\n",
    "            print(f\"{metric:25s}: {value:.4f}\")\n",
    "    \n",
    "    # Generate predictions for confusion matrix\n",
    "    print(\"\\nGenerating predictions for analysis...\")\n",
    "    predictions = base_trainer.predict(encoded_validation)\n",
    "    predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "    true_labels = predictions.label_ids\n",
    "    \n",
    "    # Create and save confusion matrix\n",
    "    visualize_confusion_matrix(\n",
    "        predicted_labels, \n",
    "        true_labels, \n",
    "        output_path=f\"{viz_dir}/base_model_confusion_matrix.png\",\n",
    "        model_name=\"Base Model\"\n",
    "    )\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(true_labels, predicted_labels, target_names=['Negative', 'Positive'])\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Save report to file\n",
    "    with open(f\"{results_dir}/base_model_classification_report.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    roc_auc = plot_roc_curve(\n",
    "        base_model, \n",
    "        encoded_validation, \n",
    "        tokenizer, \n",
    "        device, \n",
    "        output_path=f\"{viz_dir}/base_model_roc_curve.png\",\n",
    "        model_name=\"Base Model\"\n",
    "    )\n",
    "    \n",
    "    # Add ROC AUC to results\n",
    "    base_model_results['eval_roc_auc'] = roc_auc\n",
    "    \n",
    "    # Store results for later comparison\n",
    "    all_results['Base Model'] = base_model_results\n",
    "    \n",
    "    # Create confidence distribution visualization\n",
    "    print(\"\\nVisualizing prediction confidence distribution...\")\n",
    "    softmax_outputs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1)\n",
    "    confidence_scores = softmax_outputs.max(dim=1).values.numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(confidence_scores, bins=20, kde=True)\n",
    "    plt.title('Distribution of Prediction Confidence (Base Model)', fontsize=14)\n",
    "    plt.xlabel('Confidence Score', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(f\"{viz_dir}/base_model_confidence.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\nBase model evaluation complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error during base model evaluation: {e}\")\n",
    "    traceback.print_exc()\n",
    "    raise RuntimeError(\"Base model evaluation failed. Cannot proceed with PEFT.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7254f861",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db14464",
   "metadata": {},
   "source": [
    "## LoRA set up \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c259dded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring LoRA for parameter-efficient fine-tuning...\n",
      "\n",
      "Trainable parameter analysis:\n",
      "Total parameters: 68,136,964\n",
      "Trainable parameters: 1,774,084\n",
      "Percentage of parameters being trained: 2.6037%\n",
      "Parameter reduction factor: 38.41x\n",
      "\n",
      "Memory usage:\n",
      "Base model: 3076.95 MB allocated\n",
      "LoRA model: 3081.46 MB allocated\n",
      "Memory reduction: -4.51 MB (-0.15%)\n",
      "Parameter efficiency visualization saved to ./peft_output/visualizations/lora_parameter_efficiency.png\n",
      "\n",
      "LoRA model trainable parameters:\n",
      "trainable params: 1,774,084 || all params: 68,136,964 || trainable%: 2.6037027420241383\n",
      "\n",
      "LoRA configuration saved to ./peft_output/lora_config.json\n",
      "\n",
      "LoRA adapter added to the following modules:\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.k_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.out_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.out_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.out_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.out_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.out_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.out_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.out_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.0.attention.out_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.k_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.out_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.out_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.out_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.out_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.out_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.out_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.out_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.1.attention.out_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.k_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.out_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.out_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.out_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.out_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.out_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.out_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.out_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.2.attention.out_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.k_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.out_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.out_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.out_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.out_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.out_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.out_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.out_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.3.attention.out_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.k_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.out_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.out_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.out_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.out_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.out_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.out_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.out_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.4.attention.out_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_embedding_B\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.out_lin.lora_dropout\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.out_lin.lora_dropout.default\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.out_lin.lora_A\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.out_lin.lora_A.default\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.out_lin.lora_B\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.out_lin.lora_B.default\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.out_lin.lora_embedding_A\n",
      "  base_model.model.distilbert.transformer.layer.5.attention.out_lin.lora_embedding_B\n",
      "\n",
      "LoRA model setup complete with 192 adapter modules!\n"
     ]
    }
   ],
   "source": [
    "# Configure and set up LoRA for parameter-efficient fine-tuning\n",
    "\n",
    "print(\"Configuring LoRA for parameter-efficient fine-tuning...\")\n",
    "try:\n",
    "    # Define LoRA Configuration for the model\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,         # Sequence classification task\n",
    "        r=config['lora_r'],                 # Rank of the update matrices\n",
    "        lora_alpha=config['lora_alpha'],    # Scaling factor for LoRA\n",
    "        lora_dropout=config['lora_dropout'],# Dropout probability for LoRA layers\n",
    "        # Target specific attention modules\n",
    "        target_modules=[\n",
    "            \"q_lin\",                        # Query projection\n",
    "            \"v_lin\",                        # Value projection \n",
    "            \"k_lin\",                        # Key projection\n",
    "            \"out_lin\"                       # Output projection\n",
    "        ],\n",
    "        bias=\"none\",                        # Don't train bias parameters\n",
    "        inference_mode=False,               # We're training, not just inferring\n",
    "    )\n",
    "    \n",
    "    # Create the PEFT model from base model\n",
    "    peft_model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    # Print trainable vs total parameters\n",
    "    trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "    percentage = 100 * trainable_params / total_params\n",
    "    \n",
    "    print(\"\\nTrainable parameter analysis:\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Percentage of parameters being trained: {percentage:.4f}%\")\n",
    "    print(f\"Parameter reduction factor: {total_params / trainable_params:.2f}x\")\n",
    "    \n",
    "    # Record memory usage after creating PEFT model\n",
    "    lora_memory = track_memory_usage(peft_model, device)\n",
    "    \n",
    "    # Calculate memory savings\n",
    "    memory_reduction = {\n",
    "        \"allocated\": base_memory[\"allocated_mb\"] - lora_memory[\"allocated_mb\"],\n",
    "        \"reserved\": base_memory[\"reserved_mb\"] - lora_memory[\"reserved_mb\"]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nMemory usage:\")\n",
    "    print(f\"Base model: {base_memory['allocated_mb']:.2f} MB allocated\")\n",
    "    print(f\"LoRA model: {lora_memory['allocated_mb']:.2f} MB allocated\")\n",
    "    print(f\"Memory reduction: {memory_reduction['allocated']:.2f} MB ({memory_reduction['allocated']/base_memory['allocated_mb']*100:.2f}%)\")\n",
    "    \n",
    "    # Visualize parameter efficiency\n",
    "    plot_parameter_efficiency(\n",
    "        total_params, \n",
    "        trainable_params, \n",
    "        output_path=f\"{viz_dir}/lora_parameter_efficiency.png\"\n",
    "    )\n",
    "    \n",
    "    # Also use the built-in function to print parameters\n",
    "    print(\"\\nLoRA model trainable parameters:\")\n",
    "    peft_model.print_trainable_parameters()\n",
    "    \n",
    "    # Save the LoRA configuration for reference\n",
    "    with open(f\"{output_dir}/lora_config.json\", \"w\") as f:\n",
    "        # Extract config values that are JSON serializable\n",
    "        config_dict = {\n",
    "            \"task_type\": str(lora_config.task_type),\n",
    "            \"r\": lora_config.r,\n",
    "            \"lora_alpha\": lora_config.lora_alpha,\n",
    "            \"lora_dropout\": lora_config.lora_dropout,\n",
    "            \"target_modules\": lora_config.target_modules,\n",
    "            \"bias\": lora_config.bias,\n",
    "        }\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nLoRA configuration saved to {output_dir}/lora_config.json\")\n",
    "    \n",
    "    # Display model architecture changes\n",
    "    print(\"\\nLoRA adapter added to the following modules:\")\n",
    "    lora_modules = []\n",
    "    for name, module in peft_model.named_modules():\n",
    "        if 'lora' in name.lower():\n",
    "            lora_modules.append(name)\n",
    "            print(f\"  {name}\")\n",
    "    \n",
    "    print(f\"\\nLoRA model setup complete with {len(lora_modules)} adapter modules!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error setting up LoRA: {e}\")\n",
    "    traceback.print_exc()\n",
    "    raise RuntimeError(\"LoRA setup failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37e59e",
   "metadata": {},
   "source": [
    "## LoRA training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d7f7ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring training parameters...\n",
      "Creating trainer for LoRA fine-tuning...\n",
      "\n",
      "Starting LoRA fine-tuning...\n",
      "Training on 5000 examples\n",
      "Evaluating on 500 examples during training\n",
      "Training for 3 epochs with batch size 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='939' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [939/939 01:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>0.312448</td>\n",
       "      <td>0.858000</td>\n",
       "      <td>0.858012</td>\n",
       "      <td>0.861056</td>\n",
       "      <td>0.858000</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.895397</td>\n",
       "      <td>0.857715</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.823755</td>\n",
       "      <td>0.858283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.252100</td>\n",
       "      <td>0.329994</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.863908</td>\n",
       "      <td>0.864066</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.866953</td>\n",
       "      <td>0.845188</td>\n",
       "      <td>0.855932</td>\n",
       "      <td>0.861423</td>\n",
       "      <td>0.881226</td>\n",
       "      <td>0.871212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.140900</td>\n",
       "      <td>0.422410</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.856023</td>\n",
       "      <td>0.856074</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.846473</td>\n",
       "      <td>0.853556</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.858238</td>\n",
       "      <td>0.861538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./peft_output/lora_model/checkpoint-313 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./peft_output/lora_model/checkpoint-626 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./peft_output/lora_model/checkpoint-939 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 00:01:05\n",
      "Training loss: 0.2485\n",
      "\n",
      "Evaluating the fine-tuned model on the full validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LoRA fine-tuned model results:\n",
      "--------------------------------------------------\n",
      "eval_loss                : 0.3263\n",
      "eval_accuracy            : 0.8727\n",
      "eval_f1                  : 0.8726\n",
      "eval_precision           : 0.8736\n",
      "eval_recall              : 0.8727\n",
      "eval_precision_class_0   : 0.8914\n",
      "eval_recall_class_0      : 0.8435\n",
      "eval_f1_class_0          : 0.8667\n",
      "eval_precision_class_1   : 0.8565\n",
      "eval_recall_class_1      : 0.9009\n",
      "eval_f1_class_1          : 0.8782\n",
      "eval_steps_per_second    : 22.3230\n",
      "epoch                    : 3.0000\n",
      "\n",
      "LoRA PEFT model saved to ./peft_output/models/lora_model\n",
      "\n",
      "Generating learning curves...\n",
      "Training history plot saved to ./peft_output/visualizations/lora_learning_curves.png\n",
      "\n",
      "Generating predictions for analysis...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.84      0.87       428\n",
      "    Positive       0.86      0.90      0.88       444\n",
      "\n",
      "    accuracy                           0.87       872\n",
      "   macro avg       0.87      0.87      0.87       872\n",
      "weighted avg       0.87      0.87      0.87       872\n",
      "\n",
      "ROC curve saved to ./peft_output/visualizations/lora_model_roc_curve.png\n",
      "\n",
      "LoRA model training and evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Train the LoRA model\n",
    "\n",
    "print(\"Configuring training parameters...\")\n",
    "try:\n",
    "    train_args = TrainingArguments(\n",
    "        # Output settings\n",
    "        output_dir=f\"{output_dir}/lora_model\",     # Directory to save outputs\n",
    "        \n",
    "        # Training hyperparameters\n",
    "        learning_rate=config['learning_rate'],     # Learning rate\n",
    "        per_device_train_batch_size=config['batch_size'],  # Batch size for training\n",
    "        per_device_eval_batch_size=config['eval_batch_size'],  # Batch size for evaluation\n",
    "        num_train_epochs=config['epochs'],         # Number of training epochs\n",
    "        weight_decay=config['weight_decay'],       # Weight decay for regularization\n",
    "        \n",
    "        # Training loop settings\n",
    "        logging_dir=f\"{output_dir}/logs\",          # Directory for logs\n",
    "        logging_steps=50,                          # Log every 50 steps\n",
    "        save_strategy=\"epoch\",                     # Save at the end of each epoch\n",
    "        evaluation_strategy=\"epoch\",               # Evaluate at the end of each epoch\n",
    "        \n",
    "        # Optimization settings\n",
    "        gradient_accumulation_steps=1,             # Accumulate gradients over steps \n",
    "        fp16=torch.cuda.is_available(),            # Use mixed precision if GPU available\n",
    "        \n",
    "        # Model selection\n",
    "        load_best_model_at_end=True,               # Load the best model when finished\n",
    "        metric_for_best_model=\"accuracy\",          # Use accuracy to determine best model\n",
    "        greater_is_better=True,                    # Higher accuracy is better\n",
    "        \n",
    "        # Miscellaneous\n",
    "        push_to_hub=False,                         # Don't push to HuggingFace Hub\n",
    "        report_to=\"none\",                          # Disable wandb reporting\n",
    "    )\n",
    "    \n",
    "    # Create a Trainer for fine-tuning\n",
    "    print(\"Creating trainer for LoRA fine-tuning...\")\n",
    "    peft_trainer = Trainer(\n",
    "        model=peft_model,                          # The LoRA-equipped model\n",
    "        args=train_args,                           # Training arguments\n",
    "        train_dataset=encoded_train,               # Training dataset\n",
    "        eval_dataset=encoded_validation_subset,    # Evaluation dataset\n",
    "        compute_metrics=compute_metrics,           # Evaluation metrics function\n",
    "    )\n",
    "    \n",
    "    # Log the training start\n",
    "    print(\"\\nStarting LoRA fine-tuning...\")\n",
    "    print(f\"Training on {len(encoded_train)} examples\")\n",
    "    print(f\"Evaluating on {len(encoded_validation_subset)} examples during training\")\n",
    "    print(f\"Training for {train_args.num_train_epochs} epochs with batch size {train_args.per_device_train_batch_size}\")\n",
    "    \n",
    "    # Train the model with timing\n",
    "    start_time = time.time()\n",
    "    train_result = peft_trainer.train()\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Print training statistics\n",
    "    print(f\"\\nTraining completed in {format_time(training_time)}\")\n",
    "    print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate the fine-tuned model on the entire validation set\n",
    "    print(\"\\nEvaluating the fine-tuned model on the full validation set...\")\n",
    "    peft_trainer.eval_dataset = encoded_validation  # Switch to full validation set\n",
    "    lora_eval_results = peft_trainer.evaluate()\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nLoRA fine-tuned model results:\")\n",
    "    print(\"-\" * 50)\n",
    "    for metric, value in lora_eval_results.items():\n",
    "        # Skip metrics that are not relevant for display\n",
    "        if not metric.startswith('eval_runtime') and not metric.startswith('eval_samples_per'):\n",
    "            print(f\"{metric:25s}: {value:.4f}\")\n",
    "    \n",
    "    # Store results for comparison\n",
    "    all_results['LoRA Model'] = lora_eval_results\n",
    "    \n",
    "    # Save the trained PEFT model\n",
    "    peft_model_path = f\"{models_dir}/lora_model\"\n",
    "    peft_model.save_pretrained(peft_model_path)\n",
    "    print(f\"\\nLoRA PEFT model saved to {peft_model_path}\")\n",
    "    \n",
    "    # Create learning curves\n",
    "    print(\"\\nGenerating learning curves...\")\n",
    "    if hasattr(peft_trainer, 'state') and hasattr(peft_trainer.state, 'log_history'):\n",
    "        plot_training_history(\n",
    "            peft_trainer.state.log_history,\n",
    "            output_path=f\"{viz_dir}/lora_learning_curves.png\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"No training history found to generate learning curves.\")\n",
    "    \n",
    "    # Generate predictions for confusion matrix\n",
    "    print(\"\\nGenerating predictions for analysis...\")\n",
    "    predictions = peft_trainer.predict(encoded_validation)\n",
    "    predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "    true_labels = predictions.label_ids\n",
    "    \n",
    "    # Create and save confusion matrix\n",
    "    visualize_confusion_matrix(\n",
    "        predicted_labels, \n",
    "        true_labels, \n",
    "        output_path=f\"{viz_dir}/lora_model_confusion_matrix.png\",\n",
    "        model_name=\"LoRA Model\"\n",
    "    )\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(true_labels, predicted_labels, target_names=['Negative', 'Positive'])\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Save report to file\n",
    "    with open(f\"{results_dir}/lora_model_classification_report.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    roc_auc = plot_roc_curve(\n",
    "        peft_model, \n",
    "        encoded_validation, \n",
    "        tokenizer, \n",
    "        device, \n",
    "        output_path=f\"{viz_dir}/lora_model_roc_curve.png\",\n",
    "        model_name=\"LoRA Model\"\n",
    "    )\n",
    "    \n",
    "    # Add ROC AUC to results\n",
    "    lora_eval_results['eval_roc_auc'] = roc_auc\n",
    "    \n",
    "    print(\"\\nLoRA model training and evaluation complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error during LoRA training: {e}\")\n",
    "    traceback.print_exc()\n",
    "    raise RuntimeError(\"LoRA training failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fa7158",
   "metadata": {},
   "source": [
    "## QLoRA set up and training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df54d4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Setting up QLoRA (Quantized LoRA) implementation...\n",
      "================================================================================\n",
      "bitsandbytes is available - version information may vary.\n",
      "\n",
      "Creating a fresh model for QLoRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for 8-bit training...\n",
      "\n",
      "QLoRA model trainable parameters:\n",
      "trainable params: 1,331,716 || all params: 67,694,596 || trainable%: 1.967241225577297\n",
      "\n",
      "Memory usage comparison:\n",
      "Base model: 3076.95 MB allocated\n",
      "LoRA model: 3081.46 MB allocated\n",
      "QLoRA model: 2701.94 MB allocated\n",
      "QLoRA memory savings vs base: 375.02 MB (12.19%)\n",
      "Creating QLoRA trainer...\n",
      "\n",
      "Training the QLoRA model...\n",
      "Training on 2000 examples\n",
      "Evaluating on 500 examples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 02:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.395100</td>\n",
       "      <td>0.382369</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.833758</td>\n",
       "      <td>0.834328</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.803347</td>\n",
       "      <td>0.822270</td>\n",
       "      <td>0.827206</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.844278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.313600</td>\n",
       "      <td>0.375885</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.831344</td>\n",
       "      <td>0.833955</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.778243</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.812721</td>\n",
       "      <td>0.881226</td>\n",
       "      <td>0.845588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.296800</td>\n",
       "      <td>0.362949</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.846013</td>\n",
       "      <td>0.846033</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.841004</td>\n",
       "      <td>0.839248</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.850575</td>\n",
       "      <td>0.852207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./peft_output/qlora_model/checkpoint-250 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./peft_output/qlora_model/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./peft_output/qlora_model/checkpoint-750 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QLoRA training completed in 00:02:16\n",
      "\n",
      "Evaluating QLoRA model on full validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QLoRA model results:\n",
      "--------------------------------------------------\n",
      "eval_loss                : 0.3598\n",
      "eval_accuracy            : 0.8452\n",
      "eval_f1                  : 0.8452\n",
      "eval_precision           : 0.8453\n",
      "eval_recall              : 0.8452\n",
      "eval_precision_class_0   : 0.8383\n",
      "eval_recall_class_0      : 0.8481\n",
      "eval_f1_class_0          : 0.8432\n",
      "eval_precision_class_1   : 0.8519\n",
      "eval_recall_class_1      : 0.8423\n",
      "eval_f1_class_1          : 0.8471\n",
      "eval_steps_per_second    : 15.8850\n",
      "epoch                    : 3.0000\n",
      "\n",
      "QLoRA model saved to ./peft_output/models/qlora_model\n",
      "Training history plot saved to ./peft_output/visualizations/qlora_learning_curves.png\n",
      "\n",
      "Generating predictions for analysis...\n",
      "\n",
      "QLoRA implementation and training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Attempt to implement QLoRA with proper error handling\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Setting up QLoRA (Quantized LoRA) implementation...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Check if bitsandbytes is available\n",
    "    try:\n",
    "        import bitsandbytes as bnb\n",
    "        print(\"bitsandbytes is available - version information may vary.\")\n",
    "        bnb_available = True\n",
    "    except ImportError:\n",
    "        print(\"bitsandbytes is not installed. Installing now...\")\n",
    "        !pip install --user bitsandbytes==0.38.1 --quiet\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "            print(\"Successfully installed bitsandbytes!\")\n",
    "            bnb_available = True\n",
    "        except ImportError:\n",
    "            print(\"Failed to install bitsandbytes. QLoRA will not be available.\")\n",
    "            bnb_available = False\n",
    "    \n",
    "    if not bnb_available:\n",
    "        raise ImportError(\"bitsandbytes is required for QLoRA but is not available.\")\n",
    "    \n",
    "    # Check if we have GPU support\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Warning: CUDA is not available. QLoRA may not work as expected.\")\n",
    "    \n",
    "    print(\"\\nCreating a fresh model for QLoRA...\")\n",
    "    \n",
    "    # Load a fresh model for QLoRA\n",
    "    qlora_base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config['model_name'],\n",
    "        num_labels=2,\n",
    "        load_in_8bit=True  # Enable 8-bit quantization\n",
    "    )\n",
    "    \n",
    "    # Crucial step: Prepare model for 8-bit training to prevent FP16 gradient issues\n",
    "    print(\"Preparing model for 8-bit training...\")\n",
    "    qlora_base_model = prepare_model_for_kbit_training(qlora_base_model)\n",
    "    \n",
    "    # Define QLoRA configuration with more conservative settings\n",
    "    qlora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=8,                          # Lower rank for QLoRA\n",
    "        lora_alpha=16,                # Lower alpha\n",
    "        lora_dropout=0.05,            # Less dropout for stability\n",
    "        # Target fewer modules to reduce complexity\n",
    "        target_modules=[\"q_lin\", \"v_lin\"],\n",
    "        bias=\"none\",\n",
    "        inference_mode=False,\n",
    "    )\n",
    "    \n",
    "    # Create the QLoRA model\n",
    "    qlora_model = get_peft_model(qlora_base_model, qlora_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    print(\"\\nQLoRA model trainable parameters:\")\n",
    "    qlora_model.print_trainable_parameters()\n",
    "    \n",
    "    # Record memory usage\n",
    "    qlora_memory = track_memory_usage(qlora_model, device)\n",
    "    \n",
    "    # Calculate memory savings compared to base model\n",
    "    memory_savings = {\n",
    "        \"allocated\": base_memory[\"allocated_mb\"] - qlora_memory[\"allocated_mb\"],\n",
    "        \"reserved\": base_memory[\"reserved_mb\"] - qlora_memory[\"reserved_mb\"]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nMemory usage comparison:\")\n",
    "    print(f\"Base model: {base_memory['allocated_mb']:.2f} MB allocated\")\n",
    "    print(f\"LoRA model: {lora_memory['allocated_mb']:.2f} MB allocated\")\n",
    "    print(f\"QLoRA model: {qlora_memory['allocated_mb']:.2f} MB allocated\")\n",
    "    print(f\"QLoRA memory savings vs base: {memory_savings['allocated']:.2f} MB ({memory_savings['allocated']/base_memory['allocated_mb']*100:.2f}%)\")\n",
    "    \n",
    "    # Set up QLoRA training arguments - critical settings for stability\n",
    "    qlora_train_args = TrainingArguments(\n",
    "        output_dir=f\"{output_dir}/qlora_model\",\n",
    "        learning_rate=config['qlora_learning_rate'],  # Lower learning rate\n",
    "        per_device_train_batch_size=config['qlora_batch_size'],  # Smaller batch size\n",
    "        per_device_eval_batch_size=config['qlora_batch_size'],\n",
    "        num_train_epochs=config['epochs'],\n",
    "        weight_decay=0.0,           # No weight decay for stability\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "        # CRITICAL: Disable mixed precision which causes issues with 8-bit models\n",
    "        fp16=False,\n",
    "        bf16=False\n",
    "    )\n",
    "    \n",
    "    # Create a Trainer for QLoRA\n",
    "    print(\"Creating QLoRA trainer...\")\n",
    "    qlora_trainer = Trainer(\n",
    "        model=qlora_model,\n",
    "        args=qlora_train_args,\n",
    "        train_dataset=encoded_train.select(range(min(2000, len(encoded_train)))),  # Smaller subset for stability\n",
    "        eval_dataset=encoded_validation_subset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Train the QLoRA model\n",
    "    print(\"\\nTraining the QLoRA model...\")\n",
    "    print(f\"Training on {len(encoded_train.select(range(min(2000, len(encoded_train)))))} examples\")\n",
    "    print(f\"Evaluating on {len(encoded_validation_subset)} examples\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    qlora_trainer.train()\n",
    "    qlora_training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nQLoRA training completed in {format_time(qlora_training_time)}\")\n",
    "    \n",
    "    # Evaluate on full validation set\n",
    "    print(\"\\nEvaluating QLoRA model on full validation set...\")\n",
    "    qlora_trainer.eval_dataset = encoded_validation\n",
    "    qlora_eval_results = qlora_trainer.evaluate()\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nQLoRA model results:\")\n",
    "    print(\"-\" * 50)\n",
    "    for metric, value in qlora_eval_results.items():\n",
    "        if not metric.startswith('eval_runtime') and not metric.startswith('eval_samples_per'):\n",
    "            print(f\"{metric:25s}: {value:.4f}\")\n",
    "    \n",
    "    # Store results for comparison\n",
    "    all_results['QLoRA Model'] = qlora_eval_results\n",
    "    \n",
    "    # Save QLoRA model\n",
    "    qlora_model_path = f\"{models_dir}/qlora_model\"\n",
    "    qlora_model.save_pretrained(qlora_model_path)\n",
    "    print(f\"\\nQLoRA model saved to {qlora_model_path}\")\n",
    "    \n",
    "    # Generate learning curves\n",
    "    if hasattr(qlora_trainer, 'state') and hasattr(qlora_trainer.state, 'log_history'):\n",
    "        plot_training_history(\n",
    "            qlora_trainer.state.log_history,\n",
    "            output_path=f\"{viz_dir}/qlora_learning_curves.png\"\n",
    "        )\n",
    "    \n",
    "    # Generate predictions for confusion matrix\n",
    "    print(\"\\nGenerating predictions for analysis...\")\n",
    "    predictions = qlora_trainer.predict(encoded_validation)\n",
    "    predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "    true_labels = predictions.label_ids\n",
    "    \n",
    "    # Create and save confusion matrix\n",
    "    visualize_confusion_matrix(\n",
    "        predicted_labels, \n",
    "        true_labels, \n",
    "        output_path=f\"{viz_dir}/qlora_model_confusion_matrix.png\",\n",
    "        model_name=\"QLoRA Model\"\n",
    "    )\n",
    "    \n",
    "    # QLoRA implementation succeeded\n",
    "    print(\"\\nQLoRA implementation and training completed successfully!\")\n",
    "    qlora_implemented = True\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"\\nError implementing QLoRA: {e}\")\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nContinuing with standard LoRA only.\")\n",
    "    qlora_implemented = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e58deb",
   "metadata": {},
   "source": [
    "## LoRA configurations experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b605179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Experimenting with different LoRA configurations...\n",
      "================================================================================\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing configuration: Low Rank (r=4)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "trainable params: 1,331,716 || all params: 67,694,596 || trainable%: 1.967241225577297\n",
      "Training Low Rank (r=4) variant...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.450300</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.836073</td>\n",
       "      <td>0.837458</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.810277</td>\n",
       "      <td>0.857741</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.862348</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.838583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Low Rank (r=4) variant on full validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Low Rank (r=4):\n",
      "Training time: 00:00:09\n",
      "Trainable parameters: 1,331,716 (1.9672%)\n",
      "  eval_loss: 0.3604\n",
      "  eval_accuracy: 0.8383\n",
      "  eval_f1: 0.8383\n",
      "  eval_precision: 0.8396\n",
      "  eval_recall: 0.8383\n",
      "  eval_precision_class_0: 0.8168\n",
      "  eval_recall_class_0: 0.8645\n",
      "  eval_f1_class_0: 0.8400\n",
      "  eval_precision_class_1: 0.8616\n",
      "  eval_recall_class_1: 0.8131\n",
      "  eval_f1_class_1: 0.8366\n",
      "  eval_steps_per_second: 22.0190\n",
      "  epoch: 1.0000\n",
      "Variant model saved to ./peft_output/models/variant_0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing configuration: High Rank (r=32)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "trainable params: 2,363,908 || all params: 68,726,788 || trainable%: 3.439572936247217\n",
      "Training High Rank (r=32) variant...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.424600</td>\n",
       "      <td>0.359961</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.840077</td>\n",
       "      <td>0.841121</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.816733</td>\n",
       "      <td>0.857741</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.863454</td>\n",
       "      <td>0.823755</td>\n",
       "      <td>0.843137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating High Rank (r=32) variant on full validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for High Rank (r=32):\n",
      "Training time: 00:00:09\n",
      "Trainable parameters: 2,363,908 (3.4396%)\n",
      "  eval_loss: 0.3642\n",
      "  eval_accuracy: 0.8417\n",
      "  eval_f1: 0.8417\n",
      "  eval_precision: 0.8428\n",
      "  eval_recall: 0.8417\n",
      "  eval_precision_class_0: 0.8222\n",
      "  eval_recall_class_0: 0.8645\n",
      "  eval_f1_class_0: 0.8428\n",
      "  eval_precision_class_1: 0.8626\n",
      "  eval_recall_class_1: 0.8198\n",
      "  eval_f1_class_1: 0.8406\n",
      "  eval_steps_per_second: 21.8570\n",
      "  epoch: 1.0000\n",
      "Variant model saved to ./peft_output/models/variant_1\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing configuration: Query-Only\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "trainable params: 1,331,716 || all params: 67,694,596 || trainable%: 1.967241225577297\n",
      "Training Query-Only variant...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>0.381212</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.824079</td>\n",
       "      <td>0.824580</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.805668</td>\n",
       "      <td>0.832636</td>\n",
       "      <td>0.818930</td>\n",
       "      <td>0.841897</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.828794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Query-Only variant on full validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Query-Only:\n",
      "Training time: 00:00:06\n",
      "Trainable parameters: 1,331,716 (1.9672%)\n",
      "  eval_loss: 0.3807\n",
      "  eval_accuracy: 0.8303\n",
      "  eval_f1: 0.8303\n",
      "  eval_precision: 0.8304\n",
      "  eval_recall: 0.8303\n",
      "  eval_precision_class_0: 0.8226\n",
      "  eval_recall_class_0: 0.8341\n",
      "  eval_f1_class_0: 0.8283\n",
      "  eval_precision_class_1: 0.8379\n",
      "  eval_recall_class_1: 0.8266\n",
      "  eval_f1_class_1: 0.8322\n",
      "  eval_steps_per_second: 25.6870\n",
      "  epoch: 1.0000\n",
      "Variant model saved to ./peft_output/models/variant_2\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing configuration: With Bias Training\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "trainable params: 1,792,516 || all params: 68,136,964 || trainable%: 2.630754138091624\n",
      "Training With Bias Training variant...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.440300</td>\n",
       "      <td>0.367937</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.828083</td>\n",
       "      <td>0.829119</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.804781</td>\n",
       "      <td>0.845188</td>\n",
       "      <td>0.824490</td>\n",
       "      <td>0.851406</td>\n",
       "      <td>0.812261</td>\n",
       "      <td>0.831373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating With Bias Training variant on full validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for With Bias Training:\n",
      "Training time: 00:00:09\n",
      "Trainable parameters: 1,792,516 (2.6308%)\n",
      "  eval_loss: 0.3672\n",
      "  eval_accuracy: 0.8291\n",
      "  eval_f1: 0.8291\n",
      "  eval_precision: 0.8304\n",
      "  eval_recall: 0.8291\n",
      "  eval_precision_class_0: 0.8079\n",
      "  eval_recall_class_0: 0.8551\n",
      "  eval_f1_class_0: 0.8309\n",
      "  eval_precision_class_1: 0.8520\n",
      "  eval_recall_class_1: 0.8041\n",
      "  eval_f1_class_1: 0.8273\n",
      "  eval_steps_per_second: 22.0530\n",
      "  epoch: 1.0000\n",
      "Variant model saved to ./peft_output/models/variant_3\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing configuration: Higher Dropout\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "trainable params: 1,774,084 || all params: 68,136,964 || trainable%: 2.6037027420241383\n",
      "Training Higher Dropout variant...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.444800</td>\n",
       "      <td>0.364843</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.832048</td>\n",
       "      <td>0.834255</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.801556</td>\n",
       "      <td>0.861925</td>\n",
       "      <td>0.830645</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Higher Dropout variant on full validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Higher Dropout:\n",
      "Training time: 00:00:09\n",
      "Trainable parameters: 1,774,084 (2.6037%)\n",
      "  eval_loss: 0.3638\n",
      "  eval_accuracy: 0.8314\n",
      "  eval_f1: 0.8313\n",
      "  eval_precision: 0.8333\n",
      "  eval_recall: 0.8314\n",
      "  eval_precision_class_0: 0.8061\n",
      "  eval_recall_class_0: 0.8645\n",
      "  eval_f1_class_0: 0.8343\n",
      "  eval_precision_class_1: 0.8596\n",
      "  eval_recall_class_1: 0.7995\n",
      "  eval_f1_class_1: 0.8285\n",
      "  eval_steps_per_second: 21.9420\n",
      "  epoch: 1.0000\n",
      "Variant model saved to ./peft_output/models/variant_4\n",
      "\n",
      "Comparison of different LoRA configurations:\n",
      "          config_name  trainable_params  param_efficiency  accuracy      f1  precision  recall  training_time_sec  training_time_min\n",
      "0      Low Rank (r=4)           1331716            1.9672    0.8383  0.8383     0.8396  0.8383             9.3860             0.1564\n",
      "1    High Rank (r=32)           2363908            3.4396    0.8417  0.8417     0.8428  0.8417             9.3507             0.1558\n",
      "2          Query-Only           1331716            1.9672    0.8303  0.8303     0.8304  0.8303             6.8221             0.1137\n",
      "3  With Bias Training           1792516            2.6308    0.8291  0.8291     0.8304  0.8291             9.5920             0.1599\n",
      "4      Higher Dropout           1774084            2.6037    0.8314  0.8313     0.8333  0.8314             9.5063             0.1584\n",
      "Comparison saved to ./peft_output/results/lora_variants_comparison.csv\n",
      "Comparison visualization saved to ./peft_output/visualizations/lora_variants_comparison.png\n",
      "\n",
      "Best variant was: High Rank (r=32) with accuracy 0.8417\n",
      "Radar chart comparison saved to ./peft_output/visualizations/lora_variants_radar.png\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different LoRA configurations\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Experimenting with different LoRA configurations...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Define different configurations to test\n",
    "    lora_configs = [\n",
    "        {\n",
    "            \"name\": \"Low Rank (r=4)\", \n",
    "            \"config\": LoraConfig(\n",
    "                task_type=TaskType.SEQ_CLS,\n",
    "                r=4,                      # Lower rank\n",
    "                lora_alpha=16,            # Lower alpha\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=[\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"],\n",
    "                bias=\"none\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"High Rank (r=32)\", \n",
    "            \"config\": LoraConfig(\n",
    "                task_type=TaskType.SEQ_CLS,\n",
    "                r=32,                     # Higher rank\n",
    "                lora_alpha=64,            # Higher alpha\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=[\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"],\n",
    "                bias=\"none\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Query-Only\", \n",
    "            \"config\": LoraConfig(\n",
    "                task_type=TaskType.SEQ_CLS,\n",
    "                r=16,\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "                # Only targeting query projections\n",
    "                target_modules=[\"q_lin\"],\n",
    "                bias=\"none\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"With Bias Training\", \n",
    "            \"config\": LoraConfig(\n",
    "                task_type=TaskType.SEQ_CLS,\n",
    "                r=16,\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=[\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"],\n",
    "                bias=\"lora_only\"          # Train biases along with LoRA weights\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Higher Dropout\", \n",
    "            \"config\": LoraConfig(\n",
    "                task_type=TaskType.SEQ_CLS,\n",
    "                r=16,\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.3,         # Higher dropout for regularization\n",
    "                target_modules=[\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"],\n",
    "                bias=\"none\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Store results of experiments\n",
    "    comparison_results = []\n",
    "    variant_metrics = {}\n",
    "    \n",
    "    # Test each configuration with a single epoch for efficiency\n",
    "    for config_idx, config_data in enumerate(lora_configs):\n",
    "        try:\n",
    "            print(f\"\\n{'-'*60}\")\n",
    "            print(f\"Testing configuration: {config_data['name']}\")\n",
    "            print(f\"{'-'*60}\")\n",
    "            \n",
    "            # Create a fresh copy of the model\n",
    "            test_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                config['model_name'],\n",
    "                num_labels=2\n",
    "            ).to(device)\n",
    "            \n",
    "            # Apply the PEFT configuration\n",
    "            test_peft_model = get_peft_model(test_model, config_data['config'])\n",
    "            \n",
    "            # Print trainable parameters\n",
    "            print(\"Trainable parameters:\")\n",
    "            trainable_params = test_peft_model.print_trainable_parameters()\n",
    "            \n",
    "            # Quick training (1 epoch, subset of data)\n",
    "            test_train_args = TrainingArguments(\n",
    "                output_dir=f\"{output_dir}/variant_{config_idx}\",\n",
    "                learning_rate=config['learning_rate'],\n",
    "                per_device_train_batch_size=config['batch_size'],\n",
    "                per_device_eval_batch_size=config['eval_batch_size'],\n",
    "                num_train_epochs=1,  # Just 1 epoch for quick comparison\n",
    "                weight_decay=config['weight_decay'],\n",
    "                logging_steps=100,\n",
    "                save_strategy=\"no\",  # Don't save checkpoints\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                report_to=\"none\",\n",
    "                fp16=False  # Disable for consistent comparisons\n",
    "            )\n",
    "            \n",
    "            # Create trainer\n",
    "            test_trainer = Trainer(\n",
    "                model=test_peft_model,\n",
    "                args=test_train_args,\n",
    "                train_dataset=encoded_train.select(range(min(2000, len(encoded_train)))),  # Use smaller subset\n",
    "                eval_dataset=encoded_validation_subset,\n",
    "                compute_metrics=compute_metrics,\n",
    "            )\n",
    "            \n",
    "            # Train the variant\n",
    "            print(f\"Training {config_data['name']} variant...\")\n",
    "            start_time = time.time()\n",
    "            test_trainer.train()\n",
    "            train_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate on full validation set\n",
    "            print(f\"Evaluating {config_data['name']} variant on full validation set...\")\n",
    "            test_trainer.eval_dataset = encoded_validation\n",
    "            test_results = test_trainer.evaluate()\n",
    "            \n",
    "            # Calculate trainable parameters\n",
    "            trainable_params = sum(p.numel() for p in test_peft_model.parameters() if p.requires_grad)\n",
    "            total_params = sum(p.numel() for p in test_peft_model.parameters())\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"Results for {config_data['name']}:\")\n",
    "            print(f\"Training time: {format_time(train_time)}\")\n",
    "            print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.4f}%)\")\n",
    "            for metric, value in test_results.items():\n",
    "                if not metric.startswith('eval_runtime') and not metric.startswith('eval_samples_per'):\n",
    "                    print(f\"  {metric}: {value:.4f}\")\n",
    "            \n",
    "            # Store results\n",
    "            variant_metrics[config_data['name']] = test_results\n",
    "            comparison_results.append({\n",
    "                \"config_name\": config_data['name'],\n",
    "                \"trainable_params\": trainable_params,\n",
    "                \"param_efficiency\": trainable_params/total_params*100,\n",
    "                \"accuracy\": test_results[\"eval_accuracy\"],\n",
    "                \"f1\": test_results[\"eval_f1\"],\n",
    "                \"precision\": test_results[\"eval_precision\"],\n",
    "                \"recall\": test_results[\"eval_recall\"],\n",
    "                \"training_time_sec\": train_time,\n",
    "                \"training_time_min\": train_time / 60\n",
    "            })\n",
    "            \n",
    "            # Save the variant model\n",
    "            variant_path = f\"{models_dir}/variant_{config_idx}\"\n",
    "            test_peft_model.save_pretrained(variant_path)\n",
    "            print(f\"Variant model saved to {variant_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error testing configuration {config_data['name']}: {str(e)}\")\n",
    "    \n",
    "    # Print comparison table\n",
    "    if comparison_results:\n",
    "        print(\"\\nComparison of different LoRA configurations:\")\n",
    "        config_df = pd.DataFrame(comparison_results)\n",
    "        \n",
    "        # Round numeric columns for better display\n",
    "        display_df = config_df.copy()\n",
    "        for col in display_df.columns:\n",
    "            if display_df[col].dtype in [np.float64, np.float32]:\n",
    "                display_df[col] = display_df[col].round(4)\n",
    "        \n",
    "        print(display_df)\n",
    "        \n",
    "        # Save comparison to CSV\n",
    "        config_df.to_csv(f\"{results_dir}/lora_variants_comparison.csv\", index=False)\n",
    "        print(f\"Comparison saved to {results_dir}/lora_variants_comparison.csv\")\n",
    "        \n",
    "        # Create visualization of results\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        \n",
    "        # Plot accuracy comparison\n",
    "        plt.subplot(2, 2, 1)\n",
    "        sns.barplot(x='config_name', y='accuracy', data=config_df, palette='viridis')\n",
    "        plt.title('Accuracy Comparison', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "        plt.ylabel('Accuracy', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot F1 score comparison\n",
    "        plt.subplot(2, 2, 2)\n",
    "        sns.barplot(x='config_name', y='f1', data=config_df, palette='viridis')\n",
    "        plt.title('F1 Score Comparison', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "        plt.ylabel('F1 Score', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot training time comparison\n",
    "        plt.subplot(2, 2, 3)\n",
    "        sns.barplot(x='config_name', y='training_time_min', data=config_df, palette='viridis')\n",
    "        plt.title('Training Time Comparison', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "        plt.ylabel('Training Time (min)', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot parameter efficiency comparison\n",
    "        plt.subplot(2, 2, 4)\n",
    "        sns.barplot(x='config_name', y='param_efficiency', data=config_df, palette='viridis')\n",
    "        plt.title('Parameter Efficiency (% of Total)', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "        plt.ylabel('Parameter Efficiency (%)', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{viz_dir}/lora_variants_comparison.png\")\n",
    "        plt.close()\n",
    "        print(f\"Comparison visualization saved to {viz_dir}/lora_variants_comparison.png\")\n",
    "        \n",
    "        # Add best variant to all_results for the final comparison\n",
    "        best_idx = config_df['accuracy'].argmax()\n",
    "        best_config = config_df.iloc[best_idx]['config_name']\n",
    "        all_results[f'Best Variant ({best_config})'] = variant_metrics[best_config]\n",
    "        print(f\"\\nBest variant was: {best_config} with accuracy {config_df.iloc[best_idx]['accuracy']:.4f}\")\n",
    "        \n",
    "        # Create radar chart for comprehensive comparison\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        \n",
    "        # Prepare data for radar chart\n",
    "        categories = ['Accuracy', 'F1 Score', 'Parameter Efficiency', 'Speed']\n",
    "        \n",
    "        # Normalize values for radar chart\n",
    "        max_acc = config_df['accuracy'].max()\n",
    "        max_f1 = config_df['f1'].max()\n",
    "        min_params = config_df['param_efficiency'].min()\n",
    "        max_params = config_df['param_efficiency'].max()\n",
    "        min_time = config_df['training_time_sec'].min()\n",
    "        max_time = config_df['training_time_sec'].max()\n",
    "        \n",
    "        # Create radar values (normalized from 0 to 1)\n",
    "        radar_data = {}\n",
    "        for _, row in config_df.iterrows():\n",
    "            # Normalize values to 0-1 scale\n",
    "            acc_norm = row['accuracy'] / max_acc\n",
    "            f1_norm = row['f1'] / max_f1\n",
    "            \n",
    "            # For params, we want higher efficiency (lower % is better)\n",
    "            # We normalize so that higher values are better\n",
    "            if max_params == min_params:\n",
    "                param_norm = 1.0\n",
    "            else:\n",
    "                param_norm = (row['param_efficiency'] - min_params) / (max_params - min_params)\n",
    "            \n",
    "            # For time, we want faster (lower is better)\n",
    "            # We normalize and invert so that higher values are better\n",
    "            if max_time == min_time:\n",
    "                time_norm = 1.0\n",
    "            else:\n",
    "                time_norm = 1 - ((row['training_time_sec'] - min_time) / (max_time - min_time))\n",
    "            \n",
    "            radar_data[row['config_name']] = [acc_norm, f1_norm, param_norm, time_norm]\n",
    "        \n",
    "        # Create radar chart\n",
    "        from math import pi\n",
    "        \n",
    "        # Number of variables\n",
    "        N = len(categories)\n",
    "        \n",
    "        # Compute angle for each axis\n",
    "        angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "        angles += angles[:1]  # Close the loop\n",
    "        \n",
    "        # Initialize the plot\n",
    "        ax = plt.subplot(111, polar=True)\n",
    "        \n",
    "        # Draw one axis per variable and add labels\n",
    "        plt.xticks(angles[:-1], categories, fontsize=12)\n",
    "        \n",
    "        # Draw the chart for each config\n",
    "        for i, (config_name, values) in enumerate(radar_data.items()):\n",
    "            values += values[:1]  # Close the loop\n",
    "            ax.plot(angles, values, linewidth=2, linestyle='solid', label=config_name)\n",
    "            ax.fill(angles, values, alpha=0.1)\n",
    "        \n",
    "        # Add legend\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "        plt.title('LoRA Configuration Comparison', fontsize=15)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{viz_dir}/lora_variants_radar.png\")\n",
    "        plt.close()\n",
    "        print(f\"Radar chart comparison saved to {viz_dir}/lora_variants_radar.png\")\n",
    "    else:\n",
    "        print(\"No configuration comparisons were completed successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error during LoRA configuration experiments: {e}\")\n",
    "    traceback.print_exc()\n",
    "    print(\"Continuing with main LoRA model for the rest of the analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be1a42",
   "metadata": {},
   "source": [
    "## Load and evaluate fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3b1b17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Loading all saved PEFT models...\n",
      "================================================================================\n",
      "Found base model name: distilbert-base-uncased\n",
      "Tokenizer loaded from base model: distilbert-base-uncased\n",
      "\n",
      "Loading best model from: ./peft_output/models/best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model as AutoPeftModelForSequenceClassification\n",
      "Best model loaded successfully\n",
      "\n",
      "Loading variant Low Rank (r=4) from: ./peft_output/models/variant_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Low Rank (r=4) model as AutoPeftModelForSequenceClassification\n",
      "Variant Low Rank (r=4) loaded successfully\n",
      "Parameter info for Low Rank (r=4):\n",
      "trainable params: 1,184,260 || all params: 67,694,596 || trainable%: 1.749415861791981\n",
      "\n",
      "Loading variant High Rank (r=32) from: ./peft_output/models/variant_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded High Rank (r=32) model as AutoPeftModelForSequenceClassification\n",
      "Variant High Rank (r=32) loaded successfully\n",
      "Parameter info for High Rank (r=32):\n",
      "trainable params: 1,184,260 || all params: 68,726,788 || trainable%: 1.7231417827936322\n",
      "\n",
      "Loading variant Query-Only from: ./peft_output/models/variant_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Query-Only model as AutoPeftModelForSequenceClassification\n",
      "Variant Query-Only loaded successfully\n",
      "Parameter info for Query-Only:\n",
      "trainable params: 1,184,260 || all params: 67,694,596 || trainable%: 1.749415861791981\n",
      "\n",
      "Loading variant With Bias Training from: ./peft_output/models/variant_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded With Bias Training model as AutoPeftModelForSequenceClassification\n",
      "Variant With Bias Training loaded successfully\n",
      "Parameter info for With Bias Training:\n",
      "trainable params: 1,202,692 || all params: 68,136,964 || trainable%: 1.7651094639320883\n",
      "\n",
      "Loading variant Higher Dropout from: ./peft_output/models/variant_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Higher Dropout model as AutoPeftModelForSequenceClassification\n",
      "Variant Higher Dropout loaded successfully\n",
      "Parameter info for Higher Dropout:\n",
      "trainable params: 1,184,260 || all params: 68,136,964 || trainable%: 1.738058067864603\n",
      "\n",
      "------------------------------------------------------------\n",
      "Successfully loaded 6 models:\n",
      "- Best Model\n",
      "- Low Rank (r=4)\n",
      "- High Rank (r=32)\n",
      "- Query-Only\n",
      "- With Bias Training\n",
      "- Higher Dropout\n"
     ]
    }
   ],
   "source": [
    "# Load all trained PEFT models correctly\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading all saved PEFT models...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import the correct model loading classes\n",
    "import os\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM, AutoPeftModelForSequenceClassification, PeftConfig, PeftModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "try:\n",
    "    # Dictionary to store all loaded models\n",
    "    loaded_models = {}\n",
    "    \n",
    "    # Load base model name first (to be consistent across all models)\n",
    "    base_model_name = None\n",
    "    \n",
    "    # Try to find any variant model to extract base model name\n",
    "    for config_idx in range(len(lora_configs)):\n",
    "        variant_path = f\"{models_dir}/variant_{config_idx}\"\n",
    "        if os.path.exists(variant_path):\n",
    "            try:\n",
    "                peft_config = PeftConfig.from_pretrained(variant_path)\n",
    "                base_model_name = peft_config.base_model_name_or_path\n",
    "                print(f\"Found base model name: {base_model_name}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load config from {variant_path}: {e}\")\n",
    "    \n",
    "    # Default if still not found\n",
    "    if not base_model_name:\n",
    "        if 'config' in globals() and isinstance(config, dict) and 'model_name' in config:\n",
    "            base_model_name = config['model_name']\n",
    "        else:\n",
    "            base_model_name = \"distilbert-base-uncased\"  # Default fallback\n",
    "        print(f\"Using default base model: {base_model_name}\")\n",
    "    \n",
    "    # Load tokenizer (same for all models)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    print(f\"Tokenizer loaded from base model: {base_model_name}\")\n",
    "    \n",
    "    # Ensure device is defined\n",
    "    if 'device' not in locals() and 'device' not in globals():\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Device not found, using: {device}\")\n",
    "    \n",
    "    # 1. Load the best model if it exists\n",
    "    best_model_path = f\"{models_dir}/best_model\"\n",
    "    if os.path.exists(best_model_path):\n",
    "        try:\n",
    "            print(f\"\\nLoading best model from: {best_model_path}\")\n",
    "            \n",
    "            # Load the PEFT configuration\n",
    "            peft_config = PeftConfig.from_pretrained(best_model_path)\n",
    "            \n",
    "            # Determine model type and load appropriately\n",
    "            if hasattr(peft_config, 'task_type') and peft_config.task_type == 'SEQ_CLS':\n",
    "                best_model = AutoPeftModelForSequenceClassification.from_pretrained(best_model_path).to(device)\n",
    "                print(\"Loaded best model as AutoPeftModelForSequenceClassification\")\n",
    "            else:\n",
    "                best_model = AutoPeftModelForCausalLM.from_pretrained(best_model_path).to(device)\n",
    "                print(\"Loaded best model as AutoPeftModelForCausalLM\")\n",
    "            \n",
    "            # Store in dictionary\n",
    "            loaded_models[\"Best Model\"] = best_model\n",
    "            print(\"Best model loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading best model: {e}\")\n",
    "            print(\"Continuing with other models...\")\n",
    "    \n",
    "    # 2. Load all variant models\n",
    "    for config_idx, config_data in enumerate(lora_configs):\n",
    "        variant_name = config_data['name']\n",
    "        variant_path = f\"{models_dir}/variant_{config_idx}\"\n",
    "        \n",
    "        if not os.path.exists(variant_path):\n",
    "            print(f\"\\nModel path for {variant_name} not found. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nLoading variant {variant_name} from: {variant_path}\")\n",
    "            \n",
    "            # Load the PEFT configuration\n",
    "            peft_config = PeftConfig.from_pretrained(variant_path)\n",
    "            \n",
    "            # Try loading with AutoPeftModel classes first\n",
    "            try:\n",
    "                if hasattr(peft_config, 'task_type') and peft_config.task_type == 'SEQ_CLS':\n",
    "                    variant_model = AutoPeftModelForSequenceClassification.from_pretrained(variant_path).to(device)\n",
    "                    print(f\"Loaded {variant_name} model as AutoPeftModelForSequenceClassification\")\n",
    "                else:\n",
    "                    variant_model = AutoPeftModelForCausalLM.from_pretrained(variant_path).to(device)\n",
    "                    print(f\"Loaded {variant_name} model as AutoPeftModelForCausalLM\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading with AutoPeftModel: {e}\")\n",
    "                print(\"Trying alternative loading method...\")\n",
    "                \n",
    "                # Alternative loading method using PeftModel\n",
    "                from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "                \n",
    "                if hasattr(peft_config, 'task_type') and peft_config.task_type == 'SEQ_CLS':\n",
    "                    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                        peft_config.base_model_name_or_path, \n",
    "                        num_labels=2\n",
    "                    ).to(device)\n",
    "                    variant_model = PeftModel.from_pretrained(base_model, variant_path).to(device)\n",
    "                    print(f\"Loaded {variant_name} model using PeftModel.from_pretrained (sequence classification)\")\n",
    "                else:\n",
    "                    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                        peft_config.base_model_name_or_path\n",
    "                    ).to(device)\n",
    "                    variant_model = PeftModel.from_pretrained(base_model, variant_path).to(device)\n",
    "                    print(f\"Loaded {variant_name} model using PeftModel.from_pretrained (causal LM)\")\n",
    "            \n",
    "            # Store in dictionary\n",
    "            loaded_models[variant_name] = variant_model\n",
    "            print(f\"Variant {variant_name} loaded successfully\")\n",
    "            \n",
    "            # Display parameter info if available\n",
    "            if hasattr(variant_model, 'print_trainable_parameters'):\n",
    "                print(f\"Parameter info for {variant_name}:\")\n",
    "                variant_model.print_trainable_parameters()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading variant {variant_name}: {e}\")\n",
    "            print(\"Continuing with other models...\")\n",
    "    \n",
    "    # Summary of loaded models\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(f\"Successfully loaded {len(loaded_models)} models:\")\n",
    "    for model_name in loaded_models.keys():\n",
    "        print(f\"- {model_name}\")\n",
    "    \n",
    "    if not loaded_models:\n",
    "        print(\"Warning: No models were successfully loaded.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error during model loading: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a5310f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Comparing all PEFT models with the base model...\n",
      "================================================================================\n",
      "Using encoded_validation dataset for evaluation\n",
      "Using base model: distilbert-base-uncased\n",
      "\n",
      "Evaluating the base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base model metrics:\n",
      "  eval_loss: 0.6970\n",
      "  eval_accuracy: 0.5069\n",
      "  eval_f1: 0.3425\n",
      "  eval_precision: 0.2587\n",
      "  eval_recall: 0.5069\n",
      "  eval_steps_per_second: 51.6020\n",
      "Base model total parameters: 66,955,010\n",
      "\n",
      "Evaluating Best Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model metrics:\n",
      "  eval_loss: 0.3642\n",
      "  eval_accuracy: 0.8417\n",
      "  eval_f1: 0.8417\n",
      "  eval_precision: 0.8428\n",
      "  eval_recall: 0.8417\n",
      "  eval_steps_per_second: 39.7600\n",
      "Parameter efficiency: 1,184,260 trainable parameters (1.72% of total)\n",
      "\n",
      "Evaluating Low Rank (r=4)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Low Rank (r=4) metrics:\n",
      "  eval_loss: 0.3604\n",
      "  eval_accuracy: 0.8383\n",
      "  eval_f1: 0.8383\n",
      "  eval_precision: 0.8396\n",
      "  eval_recall: 0.8383\n",
      "  eval_steps_per_second: 39.7750\n",
      "Parameter efficiency: 1,184,260 trainable parameters (1.75% of total)\n",
      "\n",
      "Evaluating High Rank (r=32)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "High Rank (r=32) metrics:\n",
      "  eval_loss: 0.3642\n",
      "  eval_accuracy: 0.8417\n",
      "  eval_f1: 0.8417\n",
      "  eval_precision: 0.8428\n",
      "  eval_recall: 0.8417\n",
      "  eval_steps_per_second: 40.6880\n",
      "Parameter efficiency: 1,184,260 trainable parameters (1.72% of total)\n",
      "\n",
      "Evaluating Query-Only...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query-Only metrics:\n",
      "  eval_loss: 0.3807\n",
      "  eval_accuracy: 0.8303\n",
      "  eval_f1: 0.8303\n",
      "  eval_precision: 0.8304\n",
      "  eval_recall: 0.8303\n",
      "  eval_steps_per_second: 48.8800\n",
      "Parameter efficiency: 1,184,260 trainable parameters (1.75% of total)\n",
      "\n",
      "Evaluating With Bias Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With Bias Training metrics:\n",
      "  eval_loss: 0.3672\n",
      "  eval_accuracy: 0.8291\n",
      "  eval_f1: 0.8291\n",
      "  eval_precision: 0.8304\n",
      "  eval_recall: 0.8291\n",
      "  eval_steps_per_second: 40.7640\n",
      "Parameter efficiency: 1,202,692 trainable parameters (1.77% of total)\n",
      "\n",
      "Evaluating Higher Dropout...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Higher Dropout metrics:\n",
      "  eval_loss: 0.3638\n",
      "  eval_accuracy: 0.8314\n",
      "  eval_f1: 0.8313\n",
      "  eval_precision: 0.8333\n",
      "  eval_recall: 0.8314\n",
      "  eval_steps_per_second: 40.0110\n",
      "Parameter efficiency: 1,184,260 trainable parameters (1.74% of total)\n",
      "\n",
      "================================================================================\n",
      "Comprehensive Model Comparison\n",
      "================================================================================\n",
      "\n",
      "Metrics comparison table:\n",
      "             Model   loss accuracy recall     f1 precision steps_per_second loss vs base (%) accuracy vs base (%) recall vs base (%) f1 vs base (%) precision vs base (%) steps_per_second vs base (%) Params trained (%)\n",
      "        Base Model 0.6970   0.5069 0.5069 0.3425    0.2587          51.6020                                                                                                                                              \n",
      "        Best Model 0.3642   0.8417 0.8417 0.8417    0.8428          39.7600          -47.75%              +66.06%            +66.06%       +145.72%              +225.79%                      -22.95%              1.72%\n",
      "    Low Rank (r=4) 0.3604   0.8383 0.8383 0.8383    0.8396          39.7750          -48.29%              +65.38%            +65.38%       +144.71%              +224.56%                      -22.92%              1.75%\n",
      "  High Rank (r=32) 0.3642   0.8417 0.8417 0.8417    0.8428          40.6880          -47.75%              +66.06%            +66.06%       +145.72%              +225.79%                      -21.15%              1.72%\n",
      "        Query-Only 0.3807   0.8303 0.8303 0.8303    0.8304          48.8800          -45.38%              +63.80%            +63.80%       +142.38%              +221.00%                       -5.27%              1.75%\n",
      "With Bias Training 0.3672   0.8291 0.8291 0.8291    0.8304          40.7640          -47.32%              +63.57%            +63.57%       +142.03%              +221.01%                      -21.00%              1.77%\n",
      "    Higher Dropout 0.3638   0.8314 0.8314 0.8313    0.8333          40.0110          -47.81%              +64.03%            +64.03%       +142.69%              +222.14%                      -22.46%              1.74%\n",
      "Metrics visualization saved to ./peft_output/visualizations/all_models_metrics.png\n",
      "Parameter efficiency visualization saved to ./peft_output/visualizations/param_efficiency.png\n",
      "Radar chart saved to ./peft_output/visualizations/radar_comparison.png\n",
      "Comparison data saved to ./peft_output/results/model_comparison.csv\n",
      "\n",
      "================================================================================\n",
      "Final Analysis and Findings\n",
      "================================================================================\n",
      "\n",
      "Best model for accuracy: Best Model (0.8417)\n",
      "  Improvement over base model: +66.06%\n",
      "\n",
      "Best model for f1: Best Model (0.8417)\n",
      "  Improvement over base model: +145.72%\n",
      "\n",
      "Best model for precision: Best Model (0.8428)\n",
      "  Improvement over base model: +225.79%\n",
      "\n",
      "Best model for recall: Best Model (0.8417)\n",
      "  Improvement over base model: +66.06%\n",
      "\n",
      "Overall best PEFT model: Best Model (accuracy: 0.8417)\n",
      "  Improved accuracy by +66.06% over base model\n",
      "  While training only 1.72% of the parameters\n",
      "\n",
      "6 out of 6 PEFT models improved on accuracy\n",
      "\n",
      "6 out of 6 PEFT models improved on f1\n",
      "\n",
      "6 out of 6 PEFT models improved on precision\n",
      "\n",
      "6 out of 6 PEFT models improved on recall\n",
      "\n",
      "Parameter Efficiency Summary:\n",
      "  Best Model: 1.72% parameters trained\n",
      "  Low Rank (r=4): 1.75% parameters trained\n",
      "  High Rank (r=32): 1.72% parameters trained\n",
      "  Query-Only: 1.75% parameters trained\n",
      "  With Bias Training: 1.77% parameters trained\n",
      "  Higher Dropout: 1.74% parameters trained\n",
      "\n",
      "================================================================================\n",
      "Conclusion and Recommendations\n",
      "================================================================================\n",
      "\n",
      "Parameter-Efficient Fine-Tuning (PEFT) shows promising results:\n",
      "\n",
      "1. For best overall performance, use 'Best Model'\n",
      "   - Accuracy: 0.8417\n",
      "   - Improvement over base model: 0.3349 (+66.06%)\n",
      "   - Trains only 1.72% of parameters\n",
      "\n",
      "2. For best parameter efficiency with good performance, use 'Best Model'\n",
      "   - Trains only 1.72% of parameters\n",
      "   - Accuracy: 0.8417\n",
      "   - Improvement over base model: 0.3349 (+66.06%)\n",
      "\n",
      "3. Recommendations for future exploration:\n",
      "   - Rank comparison: Low Rank (r=4) vs High Rank (r=32)\n",
      "     Low rank accuracy: 0.8383\n",
      "     High rank accuracy: 0.8417\n",
      "     Finding: Rank has minimal impact on performance\n",
      "     Suggestion: Use lower ranks for better efficiency\n",
      "\n",
      "Overall recommendation:\n",
      "Parameter-Efficient Fine-Tuning (PEFT) offers a valuable approach for adapting\n",
      "foundation models with minimal computational resources while maintaining or even\n",
      "improving performance. Based on this evaluation, PEFT is an effective technique\n",
      "for this task and dataset.\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Compare all trained models with the base model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Comparing all PEFT models with the base model...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "try:\n",
    "    # Ensure we have loaded models\n",
    "    if 'loaded_models' not in globals() or not loaded_models:\n",
    "        print(\"No loaded PEFT models found. Cannot proceed with comparison.\")\n",
    "        raise ValueError(\"No loaded PEFT models available\")\n",
    "    \n",
    "    # Ensure we have the validation dataset for evaluation\n",
    "    eval_dataset = None\n",
    "    if 'encoded_validation' in globals():\n",
    "        eval_dataset = encoded_validation\n",
    "        print(\"Using encoded_validation dataset for evaluation\")\n",
    "    elif 'encoded_test' in globals():\n",
    "        eval_dataset = encoded_test\n",
    "        print(\"Using encoded_test dataset for evaluation\")\n",
    "    else:\n",
    "        print(\"No evaluation dataset found. Cannot proceed with model comparison.\")\n",
    "        raise ValueError(\"No evaluation dataset available\")\n",
    "    \n",
    "    # Get base model name\n",
    "    base_model_name = None\n",
    "    for model_name, model in loaded_models.items():\n",
    "        try:\n",
    "            # Try to get config from the model\n",
    "            if hasattr(model, 'peft_config') and hasattr(model.peft_config, 'base_model_name_or_path'):\n",
    "                base_model_name = model.peft_config.base_model_name_or_path\n",
    "                break\n",
    "            elif hasattr(model, 'config') and hasattr(model.config, 'base_model_name_or_path'):\n",
    "                base_model_name = model.config.base_model_name_or_path\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # If we still don't have a base model name, use fallbacks\n",
    "    if not base_model_name:\n",
    "        if 'config' in globals() and isinstance(config, dict) and 'model_name' in config:\n",
    "            base_model_name = config['model_name']\n",
    "        else:\n",
    "            base_model_name = \"distilbert-base-uncased\"  # Default\n",
    "    \n",
    "    print(f\"Using base model: {base_model_name}\")\n",
    "    \n",
    "    # Define compute_metrics function for evaluation\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "    \n",
    "    # Create evaluation arguments\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=\"./eval_results\",\n",
    "        per_device_eval_batch_size=16,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    # Dictionary to store all evaluation results\n",
    "    all_results = {}\n",
    "    \n",
    "    # 1. Evaluate the base model\n",
    "    print(\"\\nEvaluating the base model...\")\n",
    "    \n",
    "    # Load the base model\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_name,\n",
    "        num_labels=2\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create trainer for base model\n",
    "    base_trainer = Trainer(\n",
    "        model=base_model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Evaluate base model\n",
    "    base_metrics = base_trainer.evaluate()\n",
    "    \n",
    "    # Print base model metrics\n",
    "    print(\"\\nBase model metrics:\")\n",
    "    for metric_name, value in base_metrics.items():\n",
    "        if not metric_name.startswith('eval_runtime') and not metric_name.startswith('eval_samples_per'):\n",
    "            print(f\"  {metric_name}: {value:.4f}\")\n",
    "    \n",
    "    # Store base model results\n",
    "    all_results[\"Base Model\"] = {k.replace('eval_', ''): v for k, v in base_metrics.items() \n",
    "                              if not k.startswith('eval_runtime') and not k.startswith('eval_samples_per')}\n",
    "    \n",
    "    # Calculate base model parameters\n",
    "    base_params = sum(p.numel() for p in base_model.parameters())\n",
    "    print(f\"Base model total parameters: {base_params:,}\")\n",
    "    \n",
    "    # 2. Evaluate all PEFT models\n",
    "    for model_name, model in loaded_models.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        \n",
    "        # Create trainer for this model\n",
    "        model_trainer = Trainer(\n",
    "            model=model,\n",
    "            args=eval_args,\n",
    "            eval_dataset=eval_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        model_metrics = model_trainer.evaluate()\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\n{model_name} metrics:\")\n",
    "        for metric_name, value in model_metrics.items():\n",
    "            if not metric_name.startswith('eval_runtime') and not metric_name.startswith('eval_samples_per'):\n",
    "                print(f\"  {metric_name}: {value:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        all_results[model_name] = {k.replace('eval_', ''): v for k, v in model_metrics.items() \n",
    "                                if not k.startswith('eval_runtime') and not k.startswith('eval_samples_per')}\n",
    "        \n",
    "        # Calculate trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_pct = trainable_params / total_params * 100\n",
    "        \n",
    "        print(f\"Parameter efficiency: {trainable_params:,} trainable parameters ({trainable_pct:.2f}% of total)\")\n",
    "        \n",
    "        # Add parameter info to results\n",
    "        all_results[model_name]['trainable_params'] = trainable_params\n",
    "        all_results[model_name]['total_params'] = total_params\n",
    "        all_results[model_name]['trainable_pct'] = trainable_pct\n",
    "    \n",
    "    # 3. Create comprehensive comparison table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Comprehensive Model Comparison\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get all metrics that exist in any model\n",
    "    all_metric_names = set()\n",
    "    for model_metrics in all_results.values():\n",
    "        all_metric_names.update(model_metrics.keys())\n",
    "    \n",
    "    # Remove parameter metrics for cleaner table\n",
    "    metric_names = [m for m in all_metric_names if m not in \n",
    "                    ['trainable_params', 'total_params', 'trainable_pct']]\n",
    "    \n",
    "    # Create DataFrame for comparison\n",
    "    comparison_rows = []\n",
    "    \n",
    "    # Add row for base model first\n",
    "    base_row = {\"Model\": \"Base Model\"}\n",
    "    for metric in metric_names:\n",
    "        if metric in all_results[\"Base Model\"]:\n",
    "            base_row[metric] = all_results[\"Base Model\"][metric]\n",
    "    comparison_rows.append(base_row)\n",
    "    \n",
    "    # Add rows for all PEFT models\n",
    "    for model_name, metrics in all_results.items():\n",
    "        if model_name == \"Base Model\":\n",
    "            continue\n",
    "            \n",
    "        model_row = {\"Model\": model_name}\n",
    "        \n",
    "        # Add metrics\n",
    "        for metric in metric_names:\n",
    "            if metric in metrics:\n",
    "                model_row[metric] = metrics[metric]\n",
    "                \n",
    "                # Add improvement percentage\n",
    "                if metric in all_results[\"Base Model\"]:\n",
    "                    pct_metric = f\"{metric} vs base (%)\"\n",
    "                    base_value = all_results[\"Base Model\"][metric]\n",
    "                    if base_value != 0:  # Avoid division by zero\n",
    "                        model_row[pct_metric] = (metrics[metric] - base_value) / base_value * 100\n",
    "        \n",
    "        # Add parameter efficiency info\n",
    "        if 'trainable_pct' in metrics:\n",
    "            model_row[\"Params trained (%)\"] = metrics['trainable_pct']\n",
    "        \n",
    "        comparison_rows.append(model_row)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame(comparison_rows)\n",
    "    \n",
    "    # Format for display - round numeric columns\n",
    "    display_df = comparison_df.copy()\n",
    "    for col in display_df.columns:\n",
    "        if col != \"Model\" and display_df[col].dtype in [np.float64, np.float32, np.int64, np.int32]:\n",
    "            if \"vs base\" in col:\n",
    "                display_df[col] = display_df[col].apply(lambda x: f\"{x:+.2f}%\" if not pd.isna(x) else \"\")\n",
    "            elif \"Params\" in col:\n",
    "                display_df[col] = display_df[col].apply(lambda x: f\"{x:.2f}%\" if not pd.isna(x) else \"\")\n",
    "            else:\n",
    "                display_df[col] = display_df[col].apply(lambda x: f\"{x:.4f}\" if not pd.isna(x) else \"\")\n",
    "    \n",
    "    # Display the comparison table\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(\"\\nMetrics comparison table:\")\n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # 4. Visualize the results\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        # Main performance metrics to visualize\n",
    "        main_metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
    "        main_metrics = [m for m in main_metrics if m in metric_names]\n",
    "        \n",
    "        if main_metrics:\n",
    "            # Create a figure with a subplot for each metric\n",
    "            fig, axes = plt.subplots(len(main_metrics), 1, figsize=(12, 5 * len(main_metrics)))\n",
    "            \n",
    "            # If only one metric, axes isn't an array\n",
    "            if len(main_metrics) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            # Get model names in desired order (base model first, then others)\n",
    "            model_order = [\"Base Model\"] + [m for m in all_results.keys() if m != \"Base Model\"]\n",
    "            \n",
    "            # Create color palette (different color for base model)\n",
    "            colors = ['#1f77b4'] + sns.color_palette(\"viridis\", len(model_order) - 1).as_hex()\n",
    "            \n",
    "            # Plot each metric\n",
    "            for i, metric in enumerate(main_metrics):\n",
    "                # Extract data for this metric\n",
    "                metric_values = []\n",
    "                model_names = []\n",
    "                \n",
    "                for model in model_order:\n",
    "                    if model in all_results and metric in all_results[model]:\n",
    "                        metric_values.append(all_results[model][metric])\n",
    "                        model_names.append(model)\n",
    "                \n",
    "                # Create the bar plot\n",
    "                bars = axes[i].bar(model_names, metric_values, color=colors[:len(model_names)])\n",
    "                \n",
    "                # Add labels\n",
    "                axes[i].set_title(f'{metric.capitalize()} Comparison', fontsize=16)\n",
    "                axes[i].set_ylabel(metric.capitalize(), fontsize=12)\n",
    "                \n",
    "                # Add value labels\n",
    "                for bar in bars:\n",
    "                    height = bar.get_height()\n",
    "                    axes[i].annotate(f'{height:.4f}',\n",
    "                                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                                    textcoords=\"offset points\",\n",
    "                                    ha='center', va='bottom')\n",
    "                \n",
    "                # Add improvement percentages\n",
    "                base_value = all_results[\"Base Model\"][metric]\n",
    "                for j, model in enumerate(model_names):\n",
    "                    if model != \"Base Model\":\n",
    "                        model_value = all_results[model][metric]\n",
    "                        improvement = (model_value - base_value) / base_value * 100\n",
    "                        \n",
    "                        # Add text with color coding\n",
    "                        color = 'green' if improvement > 0 else 'red'\n",
    "                        axes[i].text(j, all_results[model][metric] * 0.95, \n",
    "                                    f'{improvement:+.1f}%',\n",
    "                                    ha='center', va='top',\n",
    "                                    color=color, fontweight='bold')\n",
    "                \n",
    "                # Adjust ticks\n",
    "                axes[i].set_xticks(range(len(model_names)))\n",
    "                axes[i].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "                axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Create viz directory if it doesn't exist\n",
    "            if 'viz_dir' in globals():\n",
    "                viz_path = viz_dir\n",
    "            else:\n",
    "                viz_path = \"./visualizations\"\n",
    "                \n",
    "            os.makedirs(viz_path, exist_ok=True)\n",
    "            plt.savefig(f\"{viz_path}/all_models_metrics.png\")\n",
    "            print(f\"Metrics visualization saved to {viz_path}/all_models_metrics.png\")\n",
    "            plt.close()\n",
    "            \n",
    "            # Create parameter efficiency visualization\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Prepare data\n",
    "            model_names = [m for m in all_results.keys() if m != \"Base Model\"]\n",
    "            param_pcts = [all_results[m].get('trainable_pct', 0) for m in model_names]\n",
    "            accuracy_values = [all_results[m].get('accuracy', 0) for m in model_names]\n",
    "            \n",
    "            # Base model accuracy for reference\n",
    "            base_accuracy = all_results[\"Base Model\"].get('accuracy', 0)\n",
    "            \n",
    "            # Only create this plot if we have parameter data\n",
    "            if any(param_pcts) and len(model_names) > 0:\n",
    "                # Create scatter plot\n",
    "                plt.scatter(param_pcts, accuracy_values, s=100, c=range(len(model_names)), cmap='viridis')\n",
    "                \n",
    "                # Add labels for each point\n",
    "                for i, model in enumerate(model_names):\n",
    "                    plt.annotate(model, (param_pcts[i], accuracy_values[i]), \n",
    "                                fontsize=9, ha='right', va='bottom')\n",
    "                \n",
    "                # Add reference line for base model accuracy\n",
    "                plt.axhline(y=base_accuracy, color='r', linestyle='--', alpha=0.7,\n",
    "                           label=f'Base Model Accuracy: {base_accuracy:.4f}')\n",
    "                \n",
    "                # Labels and title\n",
    "                plt.xlabel('Percentage of Parameters Trained (%)', fontsize=12)\n",
    "                plt.ylabel('Accuracy', fontsize=12)\n",
    "                plt.title('Accuracy vs Parameter Efficiency', fontsize=16)\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{viz_path}/param_efficiency.png\")\n",
    "                print(f\"Parameter efficiency visualization saved to {viz_path}/param_efficiency.png\")\n",
    "                plt.close()\n",
    "        \n",
    "        # Create radar chart for multi-dimensional comparison\n",
    "        if len(main_metrics) >= 3:  # Only create radar chart if we have enough metrics\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            \n",
    "            # Get model names and prepare data\n",
    "            model_names = [\"Base Model\"] + [m for m in all_results.keys() if m != \"Base Model\"]\n",
    "            \n",
    "            # Normalize the data for the radar chart\n",
    "            max_values = {metric: max(all_results[model].get(metric, 0) for model in model_names) \n",
    "                        for metric in main_metrics}\n",
    "            min_values = {metric: min(all_results[model].get(metric, 0) for model in model_names) \n",
    "                        for metric in main_metrics}\n",
    "            \n",
    "            # Prepare radar chart data\n",
    "            radar_data = {}\n",
    "            for model in model_names:\n",
    "                values = []\n",
    "                for metric in main_metrics:\n",
    "                    # Normalize between 0 and 1\n",
    "                    if max_values[metric] == min_values[metric]:\n",
    "                        # Avoid division by zero\n",
    "                        norm_value = 1.0\n",
    "                    else:\n",
    "                        value = all_results[model].get(metric, 0)\n",
    "                        norm_value = (value - min_values[metric]) / (max_values[metric] - min_values[metric])\n",
    "                    values.append(norm_value)\n",
    "                radar_data[model] = values\n",
    "            \n",
    "            # Create the radar chart\n",
    "            from math import pi\n",
    "            \n",
    "            # Number of variables\n",
    "            N = len(main_metrics)\n",
    "            \n",
    "            # Compute angle for each axis\n",
    "            angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "            angles += angles[:1]  # Close the loop\n",
    "            \n",
    "            # Initialize the plot\n",
    "            ax = plt.subplot(111, polar=True)\n",
    "            \n",
    "            # Draw one axis per variable and add labels\n",
    "            plt.xticks(angles[:-1], [m.capitalize() for m in main_metrics], fontsize=12)\n",
    "            \n",
    "            # Draw the chart for each model\n",
    "            for i, (model, values) in enumerate(radar_data.items()):\n",
    "                values += values[:1]  # Close the loop\n",
    "                ax.plot(angles, values, linewidth=2, linestyle='solid', label=model)\n",
    "                ax.fill(angles, values, alpha=0.1)\n",
    "            \n",
    "            # Add legend\n",
    "            plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "            plt.title('Model Performance Comparison', fontsize=15)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{viz_path}/radar_comparison.png\")\n",
    "            print(f\"Radar chart saved to {viz_path}/radar_comparison.png\")\n",
    "            plt.close()\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"Matplotlib not available. Skipping visualizations.\")\n",
    "    \n",
    "    # 5. Save results to CSV\n",
    "    try:\n",
    "        if 'results_dir' in globals():\n",
    "            results_path = results_dir\n",
    "        else:\n",
    "            results_path = \"./results\"\n",
    "            \n",
    "        os.makedirs(results_path, exist_ok=True)\n",
    "        comparison_df.to_csv(f\"{results_path}/model_comparison.csv\", index=False)\n",
    "        print(f\"Comparison data saved to {results_path}/model_comparison.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save comparison results to CSV: {e}\")\n",
    "    \n",
    "    # 6. Final analysis and findings\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Final Analysis and Findings\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Find best model for each metric\n",
    "    for metric in main_metrics:\n",
    "        models_with_metric = [(model, metrics.get(metric, 0)) \n",
    "                             for model, metrics in all_results.items()]\n",
    "        best_model = max(models_with_metric, key=lambda x: x[1])\n",
    "        \n",
    "        print(f\"\\nBest model for {metric}: {best_model[0]} ({best_model[1]:.4f})\")\n",
    "        \n",
    "        # If best model isn't base model, show improvement\n",
    "        if best_model[0] != \"Base Model\":\n",
    "            base_value = all_results[\"Base Model\"].get(metric, 0)\n",
    "            improvement = (best_model[1] - base_value) / base_value * 100\n",
    "            print(f\"  Improvement over base model: {improvement:+.2f}%\")\n",
    "    \n",
    "    # Calculate overall best model (using accuracy)\n",
    "    if 'accuracy' in main_metrics:\n",
    "        # Get all models except base\n",
    "        peft_models = [(model, metrics.get('accuracy', 0)) \n",
    "                      for model, metrics in all_results.items() \n",
    "                      if model != \"Base Model\"]\n",
    "        \n",
    "        if peft_models:\n",
    "            overall_best = max(peft_models, key=lambda x: x[1])\n",
    "            print(f\"\\nOverall best PEFT model: {overall_best[0]} (accuracy: {overall_best[1]:.4f})\")\n",
    "            \n",
    "            # Compare to base\n",
    "            base_accuracy = all_results[\"Base Model\"].get('accuracy', 0)\n",
    "            \n",
    "            if overall_best[1] > base_accuracy:\n",
    "                improvement = (overall_best[1] - base_accuracy) / base_accuracy * 100\n",
    "                print(f\"  Improved accuracy by {improvement:+.2f}% over base model\")\n",
    "                \n",
    "                # Show parameter efficiency benefit\n",
    "                if 'trainable_pct' in all_results[overall_best[0]]:\n",
    "                    train_pct = all_results[overall_best[0]]['trainable_pct']\n",
    "                    print(f\"  While training only {train_pct:.2f}% of the parameters\")\n",
    "            else:\n",
    "                print(f\"  Note: Base model still has higher accuracy ({base_accuracy:.4f})\")\n",
    "    \n",
    "    # Count models that improved on base model\n",
    "    models_beating_base = {}\n",
    "    for metric in main_metrics:\n",
    "        beating_count = 0\n",
    "        for model, metrics in all_results.items():\n",
    "            if model == \"Base Model\":\n",
    "                continue\n",
    "            if metric in metrics and metrics[metric] > all_results[\"Base Model\"].get(metric, 0):\n",
    "                beating_count += 1\n",
    "        \n",
    "        total_peft_models = len(all_results) - 1  # Exclude base model\n",
    "        models_beating_base[metric] = beating_count\n",
    "        \n",
    "        print(f\"\\n{beating_count} out of {total_peft_models} PEFT models improved on {metric}\")\n",
    "    \n",
    "    # Parameter efficiency summary\n",
    "    print(\"\\nParameter Efficiency Summary:\")\n",
    "    for model_name, metrics in all_results.items():\n",
    "        if model_name == \"Base Model\":\n",
    "            continue\n",
    "        if 'trainable_pct' in metrics:\n",
    "            print(f\"  {model_name}: {metrics['trainable_pct']:.2f}% parameters trained\")\n",
    "    \n",
    "    # Final conclusion and recommendations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Conclusion and Recommendations\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall conclusion about PEFT effectiveness\n",
    "    if sum(models_beating_base.values()) > 0:\n",
    "        print(\"\\nParameter-Efficient Fine-Tuning (PEFT) shows promising results:\")\n",
    "        \n",
    "        # Recommendation based on best model\n",
    "        if 'accuracy' in main_metrics:\n",
    "            peft_accuracies = [(model, metrics.get('accuracy', 0)) \n",
    "                              for model, metrics in all_results.items() \n",
    "                              if model != \"Base Model\"]\n",
    "            \n",
    "            if peft_accuracies:\n",
    "                best_peft = max(peft_accuracies, key=lambda x: x[1])\n",
    "                \n",
    "                print(f\"\\n1. For best overall performance, use '{best_peft[0]}'\")\n",
    "                print(f\"   - Accuracy: {best_peft[1]:.4f}\")\n",
    "                \n",
    "                base_accuracy = all_results[\"Base Model\"].get('accuracy', 0)\n",
    "                acc_diff = best_peft[1] - base_accuracy\n",
    "                \n",
    "                if acc_diff > 0:\n",
    "                    print(f\"   - Improvement over base model: {acc_diff:.4f} ({acc_diff/base_accuracy*100:+.2f}%)\")\n",
    "                else:\n",
    "                    print(f\"   - Performance difference from base model: {acc_diff:.4f} ({acc_diff/base_accuracy*100:+.2f}%)\")\n",
    "                \n",
    "                if 'trainable_pct' in all_results[best_peft[0]]:\n",
    "                    print(f\"   - Trains only {all_results[best_peft[0]]['trainable_pct']:.2f}% of parameters\")\n",
    "        \n",
    "        # Recommendation for parameter efficiency\n",
    "        peft_models_with_params = [(model, metrics) for model, metrics in all_results.items() \n",
    "                                 if model != \"Base Model\" and 'trainable_pct' in metrics]\n",
    "        \n",
    "        if peft_models_with_params:\n",
    "            # Find model with lowest trainable percentage among those with good performance\n",
    "            threshold = 0.95  # Models with at least 95% of best accuracy\n",
    "            \n",
    "            if 'accuracy' in main_metrics:\n",
    "                best_acc = max(all_results[m].get('accuracy', 0) for m in all_results if m != \"Base Model\")\n",
    "                acceptable_models = [(m, metrics) for m, metrics in all_results.items() \n",
    "                                  if m != \"Base Model\" \n",
    "                                  and metrics.get('accuracy', 0) >= best_acc * threshold\n",
    "                                  and 'trainable_pct' in metrics]\n",
    "                \n",
    "                if acceptable_models:\n",
    "                    most_efficient = min(acceptable_models, key=lambda x: x[1]['trainable_pct'])\n",
    "                    \n",
    "                    print(f\"\\n2. For best parameter efficiency with good performance, use '{most_efficient[0]}'\")\n",
    "                    print(f\"   - Trains only {most_efficient[1]['trainable_pct']:.2f}% of parameters\")\n",
    "                    print(f\"   - Accuracy: {most_efficient[1].get('accuracy', 0):.4f}\")\n",
    "                    \n",
    "                    base_accuracy = all_results[\"Base Model\"].get('accuracy', 0)\n",
    "                    acc_diff = most_efficient[1].get('accuracy', 0) - base_accuracy\n",
    "                    \n",
    "                    if acc_diff > 0:\n",
    "                        print(f\"   - Improvement over base model: {acc_diff:.4f} ({acc_diff/base_accuracy*100:+.2f}%)\")\n",
    "                    else:\n",
    "                        print(f\"   - Performance difference from base model: {acc_diff:.4f} ({acc_diff/base_accuracy*100:+.2f}%)\")\n",
    "        \n",
    "        # Recommendation for future exploration\n",
    "        print(\"\\n3. Recommendations for future exploration:\")\n",
    "        \n",
    "        # Check if higher rank models perform better\n",
    "        try:\n",
    "            # Try to identify models with different ranks\n",
    "            rank_models = []\n",
    "            low_rank_model = None\n",
    "            high_rank_model = None\n",
    "            \n",
    "            # Look for specific model names that indicate ranks\n",
    "            for model_name in all_results:\n",
    "                if model_name == \"Base Model\":\n",
    "                    continue\n",
    "                    \n",
    "                if \"Low Rank\" in model_name:\n",
    "                    low_rank_model = model_name\n",
    "                elif \"High Rank\" in model_name:\n",
    "                    high_rank_model = model_name\n",
    "            \n",
    "            # Compare the models if we found them\n",
    "            if low_rank_model and high_rank_model:\n",
    "                low_rank_acc = all_results[low_rank_model].get('accuracy', 0)\n",
    "                high_rank_acc = all_results[high_rank_model].get('accuracy', 0)\n",
    "                \n",
    "                print(f\"   - Rank comparison: {low_rank_model} vs {high_rank_model}\")\n",
    "                print(f\"     Low rank accuracy: {low_rank_acc:.4f}\")\n",
    "                print(f\"     High rank accuracy: {high_rank_acc:.4f}\")\n",
    "                \n",
    "                if high_rank_acc > low_rank_acc * 1.01:  # At least 1% better\n",
    "                    print(\"     Finding: Higher ranks appear to improve performance\")\n",
    "                    print(\"     Suggestion: Experiment with even higher ranks\")\n",
    "                elif low_rank_acc > high_rank_acc * 1.01:  # At least 1% better\n",
    "                    print(\"     Finding: Lower ranks achieve better performance\")\n",
    "                    print(\"     Suggestion: Experiment with even lower ranks to find the optimal balance\")\n",
    "                else:\n",
    "                    print(\"     Finding: Rank has minimal impact on performance\")\n",
    "                    print(\"     Suggestion: Use lower ranks for better efficiency\")\n",
    "        except Exception as e:\n",
    "            print(f\"   - Could not analyze rank impact: {e}\")\n",
    "            print(\"     Suggestion: Experiment with different LoRA ranks manually\")\n",
    "        \n",
    "        print(\"\\nOverall recommendation:\")\n",
    "        print(\"Parameter-Efficient Fine-Tuning (PEFT) offers a valuable approach for adapting\")\n",
    "        print(\"foundation models with minimal computational resources while maintaining or even\")\n",
    "        print(\"improving performance. Based on this evaluation, PEFT is an effective technique\")\n",
    "        print(\"for this task and dataset.\")\n",
    "    else:\n",
    "        print(\"\\nNone of the PEFT models outperformed the base model on the evaluated metrics.\")\n",
    "        print(\"This could indicate that:\")\n",
    "        print(\"1. The base model is already well-suited for this task\")\n",
    "        print(\"2. The LoRA configuration parameters need further tuning\")\n",
    "        print(\"3. Different PEFT techniques may be more appropriate for this specific task\")\n",
    "        \n",
    "        print(\"\\nRecommendations:\")\n",
    "        print(\"- Experiment with higher LoRA ranks\")\n",
    "        print(\"- Try different target modules for the adapters\")\n",
    "        print(\"- Adjust learning rates or training durations\")\n",
    "        print(\"- Consider alternative PEFT methods like prompt tuning or prefix tuning\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error during model comparison: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc96905a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Performing inference with the best LoRA model...\n",
      "================================================================================\n",
      "Selected Best Model for inference\n",
      "\n",
      "Selected model for inference: Best Model\n",
      "Using base model: distilbert-base-uncased\n",
      "\n",
      "Testing model on sample texts:\n",
      "\n",
      "Text: I absolutely loved this product! It exceeded all my expectations.\n",
      "Prediction: Positive (confidence: 0.9897)\n",
      "Probability distribution: Positive=0.9897, Negative=0.0103\n",
      "\n",
      "Text: The service was terrible and I would never recommend this place.\n",
      "Prediction: Negative (confidence: 0.9428)\n",
      "Probability distribution: Positive=0.0572, Negative=0.9428\n",
      "\n",
      "Text: It was okay, not great but not bad either.\n",
      "Prediction: Positive (confidence: 0.8879)\n",
      "Probability distribution: Positive=0.8879, Negative=0.1121\n",
      "\n",
      "Text: While there were some issues, overall I had a positive experience.\n",
      "Prediction: Positive (confidence: 0.9931)\n",
      "Probability distribution: Positive=0.9931, Negative=0.0069\n",
      "\n",
      "Text: I'm requesting a refund because this product is defective.\n",
      "Prediction: Negative (confidence: 0.9624)\n",
      "Probability distribution: Positive=0.0376, Negative=0.9624\n",
      "\n",
      "Text: This is neither positive nor negative, it's just a statement of fact.\n",
      "Prediction: Negative (confidence: 0.8112)\n",
      "Probability distribution: Positive=0.1888, Negative=0.8112\n",
      "\n",
      "Demonstrating batch prediction:\n",
      "Batch results: 3 Positive, 3 Negative\n",
      "Average prediction confidence: 0.9312\n",
      "Sample predictions saved to ./peft_output/results/sample_predictions.csv\n",
      "Visualization saved to ./peft_output/visualizations/sample_predictions.png\n",
      "Best model saved to ./peft_output/models/best_model\n",
      "README file with usage instructions created at ./peft_output/models/best_model/README.md\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Inference with Best LoRA Model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Performing inference with the best LoRA model...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "try:\n",
    "    # Simply select the best model by name if available, or use the first one\n",
    "    if 'loaded_models' not in globals() or not loaded_models:\n",
    "        print(\"No loaded models found. Cannot proceed with inference.\")\n",
    "        raise ValueError(\"No loaded models available\")\n",
    "    \n",
    "    # Try to find \"High Rank\" or \"Best Model\" in loaded models\n",
    "    best_model = None\n",
    "    best_variant = None\n",
    "    \n",
    "    # Priority order for selecting model\n",
    "    priority_names = [\"Best Model\", \"High Rank (r=32)\", \"LoRA Model\"]\n",
    "    \n",
    "    for name in priority_names:\n",
    "        if name in loaded_models:\n",
    "            best_model = loaded_models[name]\n",
    "            best_variant = name\n",
    "            print(f\"Selected {name} for inference\")\n",
    "            break\n",
    "    \n",
    "    # If none of the preferred models found, use the first available\n",
    "    if not best_model:\n",
    "        best_variant = list(loaded_models.keys())[0]\n",
    "        best_model = loaded_models[best_variant]\n",
    "        print(f\"No priority model found. Using {best_variant} for inference.\")\n",
    "    \n",
    "    if best_model:\n",
    "        print(f\"\\nSelected model for inference: {best_variant}\")\n",
    "        \n",
    "        # Load tokenizer based on the model\n",
    "        model_name = None\n",
    "        try:\n",
    "            if 'config' in globals() and isinstance(config, dict) and 'model_name' in config:\n",
    "                model_name = config['model_name']\n",
    "            else:\n",
    "                # Try to extract from PeftConfig\n",
    "                for path in [f\"{models_dir}/variant_{i}\" for i in range(len(lora_configs))]:\n",
    "                    if os.path.exists(path):\n",
    "                        try:\n",
    "                            peft_config = PeftConfig.from_pretrained(path)\n",
    "                            model_name = peft_config.base_model_name_or_path\n",
    "                            break\n",
    "                        except:\n",
    "                            pass\n",
    "                \n",
    "                # Use default if still not found\n",
    "                if not model_name:\n",
    "                    model_name = \"distilbert-base-uncased\"  # Default\n",
    "        except:\n",
    "            model_name = \"distilbert-base-uncased\"  # Default\n",
    "            \n",
    "        print(f\"Using base model: {model_name}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Create a comprehensive inference function\n",
    "        def predict_sentiment(text, model=best_model, tokenizer=tokenizer, return_all=False):\n",
    "            \"\"\"\n",
    "            Predict sentiment for a given text using the best trained model\n",
    "            \n",
    "            Args:\n",
    "                text: Input text string or list of strings\n",
    "                model: Trained model\n",
    "                tokenizer: Tokenizer for the model\n",
    "                return_all: Whether to return all details or just the label\n",
    "            \n",
    "            Returns:\n",
    "                If return_all=True: Dict with prediction, confidence, and label\n",
    "                If return_all=False: Just the label string\n",
    "                For lists, returns a list of results\n",
    "            \"\"\"\n",
    "            # Handle both single texts and batches\n",
    "            is_single = isinstance(text, str)\n",
    "            texts = [text] if is_single else text\n",
    "            \n",
    "            # Prepare the inputs\n",
    "            inputs = tokenizer(\n",
    "                texts, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                max_length=512\n",
    "            ).to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            # Process outputs\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            \n",
    "            # Prepare results\n",
    "            results = []\n",
    "            for i, pred in enumerate(predictions):\n",
    "                label = \"Positive\" if pred == 1 else \"Negative\"\n",
    "                confidence = probabilities[i][pred].item()\n",
    "                \n",
    "                if return_all:\n",
    "                    results.append({\n",
    "                        \"prediction\": int(pred),\n",
    "                        \"confidence\": confidence,\n",
    "                        \"label\": label,\n",
    "                        \"probabilities\": {\n",
    "                            \"Negative\": probabilities[i][0].item(),\n",
    "                            \"Positive\": probabilities[i][1].item()\n",
    "                        }\n",
    "                    })\n",
    "                else:\n",
    "                    results.append(label)\n",
    "            \n",
    "            # Return single result or list based on input\n",
    "            return results[0] if is_single else results\n",
    "        \n",
    "        # Define a set of diverse test cases\n",
    "        test_cases = [\n",
    "            \"I absolutely loved this product! It exceeded all my expectations.\",\n",
    "            \"The service was terrible and I would never recommend this place.\",\n",
    "            \"It was okay, not great but not bad either.\",\n",
    "            \"While there were some issues, overall I had a positive experience.\",\n",
    "            \"I'm requesting a refund because this product is defective.\",\n",
    "            \"This is neither positive nor negative, it's just a statement of fact.\"\n",
    "        ]\n",
    "        \n",
    "        # Test the model on sample cases with detailed output\n",
    "        print(\"\\nTesting model on sample texts:\")\n",
    "        results_data = []\n",
    "        \n",
    "        for text in test_cases:\n",
    "            result = predict_sentiment(text, return_all=True)\n",
    "            print(f\"\\nText: {text}\")\n",
    "            print(f\"Prediction: {result['label']} (confidence: {result['confidence']:.4f})\")\n",
    "            print(f\"Probability distribution: Positive={result['probabilities']['Positive']:.4f}, \" +\n",
    "                  f\"Negative={result['probabilities']['Negative']:.4f}\")\n",
    "            \n",
    "            results_data.append({\n",
    "                \"text\": text,\n",
    "                \"prediction\": result['prediction'],\n",
    "                \"confidence\": result['confidence'],\n",
    "                \"label\": result['label'],\n",
    "                \"pos_prob\": result['probabilities']['Positive'],\n",
    "                \"neg_prob\": result['probabilities']['Negative']\n",
    "            })\n",
    "        \n",
    "        # Batch prediction demonstration\n",
    "        print(\"\\nDemonstrating batch prediction:\")\n",
    "        batch_results = predict_sentiment(test_cases, return_all=True)\n",
    "        \n",
    "        # Print batch statistics\n",
    "        pos_count = sum(1 for r in batch_results if r['label'] == 'Positive')\n",
    "        neg_count = sum(1 for r in batch_results if r['label'] == 'Negative')\n",
    "        avg_confidence = sum(r['confidence'] for r in batch_results) / len(batch_results)\n",
    "        \n",
    "        print(f\"Batch results: {pos_count} Positive, {neg_count} Negative\")\n",
    "        print(f\"Average prediction confidence: {avg_confidence:.4f}\")\n",
    "        \n",
    "        # Save the sample predictions to CSV if possible\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            sample_df = pd.DataFrame(results_data)\n",
    "            \n",
    "            # Create results directory if it doesn't exist\n",
    "            if 'results_dir' in globals():\n",
    "                results_path = results_dir\n",
    "            else:\n",
    "                results_path = \"./results\"\n",
    "                \n",
    "            os.makedirs(results_path, exist_ok=True)\n",
    "            sample_df.to_csv(f\"{results_path}/sample_predictions.csv\", index=False)\n",
    "            print(f\"Sample predictions saved to {results_path}/sample_predictions.csv\")\n",
    "            \n",
    "            # Create visualization of the results if matplotlib is available\n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "                import numpy as np\n",
    "                \n",
    "                # Set up the figure\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                \n",
    "                # Create bar chart of confidence scores\n",
    "                texts_short = [t[:30] + \"...\" if len(t) > 30 else t for t in sample_df['text']]\n",
    "                indices = np.arange(len(texts_short))\n",
    "                bar_width = 0.35\n",
    "                \n",
    "                # Plot positive and negative probabilities\n",
    "                plt.bar(indices, sample_df['pos_prob'], bar_width, \n",
    "                        label='Positive', color='green', alpha=0.7)\n",
    "                plt.bar(indices + bar_width, sample_df['neg_prob'], bar_width,\n",
    "                        label='Negative', color='red', alpha=0.7)\n",
    "                \n",
    "                # Add labels and title\n",
    "                plt.xlabel('Sample Text')\n",
    "                plt.ylabel('Probability')\n",
    "                plt.title('Sentiment Analysis Results')\n",
    "                plt.xticks(indices + bar_width/2, texts_short, rotation=45, ha='right')\n",
    "                plt.legend()\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Create viz directory if it doesn't exist\n",
    "                if 'viz_dir' in globals():\n",
    "                    viz_path = viz_dir\n",
    "                else:\n",
    "                    viz_path = \"./visualizations\"\n",
    "                    \n",
    "                os.makedirs(viz_path, exist_ok=True)\n",
    "                plt.savefig(f\"{viz_path}/sample_predictions.png\")\n",
    "                plt.close()\n",
    "                print(f\"Visualization saved to {viz_path}/sample_predictions.png\")\n",
    "                \n",
    "            except ImportError:\n",
    "                print(\"Matplotlib not available. Skipping visualization.\")\n",
    "                \n",
    "        except ImportError:\n",
    "            print(\"Pandas not available. CSV export skipped.\")\n",
    "        \n",
    "        # Save the best model in a separate directory for easy reference\n",
    "        try:\n",
    "            if 'models_dir' in globals():\n",
    "                models_path = models_dir\n",
    "            else:\n",
    "                models_path = \"./models\"\n",
    "                \n",
    "            best_model_path = f\"{models_path}/best_model\"\n",
    "            os.makedirs(best_model_path, exist_ok=True)\n",
    "            best_model.save_pretrained(best_model_path)\n",
    "            print(f\"Best model saved to {best_model_path}\")\n",
    "            \n",
    "            # Create a simple README with usage instructions\n",
    "            readme_content = f\"\"\"# LoRA Fine-tuned Sentiment Analysis Model\n",
    "\n",
    "## Model Information\n",
    "- Base model: {model_name}\n",
    "- Configuration: {best_variant}\n",
    "- Date: {time.strftime(\"%Y-%m-%d\")}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load model\n",
    "peft_config = PeftConfig.from_pretrained(\"./best_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    num_labels=2\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, \"./best_model\")\n",
    "\n",
    "# Predict sentiment\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=-1).item()\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\"\n",
    "```\n",
    "\"\"\"\n",
    "            \n",
    "            with open(f\"{best_model_path}/README.md\", \"w\") as f:\n",
    "                f.write(readme_content)\n",
    "                \n",
    "            print(f\"README file with usage instructions created at {best_model_path}/README.md\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving best model: {e}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No models available for inference.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error during inference with best model: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3888282c",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Generating comprehensive visualizations of results...\n",
      "================================================================================\n",
      "\n",
      "Generating model performance comparisons...\n",
      "Saved accuracy comparison chart to ./peft_output/visualizations/accuracy_comparison.png\n",
      "Saved f1 comparison chart to ./peft_output/visualizations/f1_comparison.png\n",
      "Saved precision comparison chart to ./peft_output/visualizations/precision_comparison.png\n",
      "Saved recall comparison chart to ./peft_output/visualizations/recall_comparison.png\n",
      "\n",
      "Generating parameter efficiency visualizations...\n",
      "\n",
      "Generating training curves...\n",
      "\n",
      "Generating LoRA configuration comparisons...\n",
      "\n",
      "Generating memory usage comparison...\n",
      "\n",
      "Generating comprehensive comparison...\n",
      "Saved comprehensive comparison to ./peft_output/visualizations/comprehensive_comparison.png\n",
      "\n",
      "Generating sample predictions visualization...\n",
      "Saved sample predictions visualization to ./peft_output/visualizations/sample_predictions_confidence.png\n",
      "Saved probability distribution visualization to ./peft_output/visualizations/sample_predictions_probabilities.png\n",
      "\n",
      "Generating confusion matrices for all models...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Best Model confusion matrix to ./peft_output/visualizations/best_model_confusion_matrix.png\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Low Rank (r=4) confusion matrix to ./peft_output/visualizations/low_rank_(r=4)_confusion_matrix.png\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved High Rank (r=32) confusion matrix to ./peft_output/visualizations/high_rank_(r=32)_confusion_matrix.png\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Query-Only confusion matrix to ./peft_output/visualizations/query-only_confusion_matrix.png\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved With Bias Training confusion matrix to ./peft_output/visualizations/with_bias_training_confusion_matrix.png\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Higher Dropout confusion matrix to ./peft_output/visualizations/higher_dropout_confusion_matrix.png\n",
      "\n",
      "All visualizations complete! Check the output directory: ./peft_output/visualizations\n",
      "\n",
      "Created 42 visualization files:\n",
      "1. accuracy_comparison.png\n",
      "2. accuracy_vs_time.png\n",
      "3. all_models_metrics.png\n",
      "4. base_model_confidence.png\n",
      "5. base_model_confusion_matrix.png\n",
      "6. base_model_roc_curve.png\n",
      "7. best_model_confusion_matrix.png\n",
      "8. class_distribution.png\n",
      "9. comprehensive_comparison.png\n",
      "10. confusion_matrices.png\n",
      "11. example_model_comparison.png\n",
      "12. example_param_efficiency.png\n",
      "13. f1_comparison.png\n",
      "14. high_rank_(r=32)_confusion_matrix.png\n",
      "15. higher_dropout_confusion_matrix.png\n",
      "16. inference_confidence.png\n",
      "17. learning_curves.png\n",
      "18. lora_learning_curves.png\n",
      "19. lora_model_confusion_matrix.png\n",
      "20. lora_model_roc_curve.png\n",
      "21. lora_parameter_efficiency.png\n",
      "22. lora_variants_comparison.png\n",
      "23. lora_variants_radar.png\n",
      "24. low_rank_(r=4)_confusion_matrix.png\n",
      "25. metrics_heatmap.png\n",
      "26. model_comparison.png\n",
      "27. parallel_coordinates_metrics.png\n",
      "28. param_efficiency.png\n",
      "29. parameter_efficiency.png\n",
      "30. precision_comparison.png\n",
      "31. prediction_distribution.png\n",
      "32. qlora_learning_curves.png\n",
      "33. qlora_model_confusion_matrix.png\n",
      "34. query-only_confusion_matrix.png\n",
      "35. radar_comparison.png\n",
      "36. recall_comparison.png\n",
      "37. sample_predictions.png\n",
      "38. sample_predictions_confidence.png\n",
      "39. sample_predictions_probabilities.png\n",
      "40. simulated_confusion_matrices.png\n",
      "41. simulated_prediction_distribution.png\n",
      "42. with_bias_training_confusion_matrix.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualizations\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Generating comprehensive visualizations of results...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Make sure visualization directories exist\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "# 1. Compare all model performances\n",
    "def plot_model_comparison(results_dict, metrics=['accuracy', 'f1', 'precision', 'recall']):\n",
    "    \"\"\"Create comparison charts for different models on multiple metrics\"\"\"\n",
    "    \n",
    "    # Filter out metrics that don't start with eval_\n",
    "    filtered_metrics = [m for m in metrics if f'eval_{m}' in next(iter(results_dict.values()))]\n",
    "    \n",
    "    for metric in filtered_metrics:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Sort models for consistency, with Base Model always first\n",
    "        models = [\"Base Model\"] + [m for m in results_dict.keys() if m != \"Base Model\"]\n",
    "        values = [results_dict[model][f'eval_{metric}'] for model in models]\n",
    "        \n",
    "        # Create bar chart with custom colors\n",
    "        colors = ['#1f77b4'] + sns.color_palette(\"viridis\", len(models)-1)\n",
    "        bars = plt.bar(models, values, color=colors)\n",
    "        \n",
    "        # Set title and labels\n",
    "        plt.title(f'{metric.capitalize()} Comparison Across Models', fontsize=14)\n",
    "        plt.xlabel('Model', fontsize=12)\n",
    "        plt.ylabel(metric.capitalize(), fontsize=12)\n",
    "        \n",
    "        # Add value labels on top of each bar\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                     f'{height:.4f}',\n",
    "                     ha='center', va='bottom', fontsize=11)\n",
    "        \n",
    "        # Add improvement percentages relative to base model\n",
    "        base_value = results_dict[\"Base Model\"][f'eval_{metric}']\n",
    "        for i, model in enumerate(models):\n",
    "            if model != \"Base Model\":\n",
    "                model_value = results_dict[model][f'eval_{metric}']\n",
    "                improvement = (model_value - base_value) / base_value * 100\n",
    "                color = 'green' if improvement > 0 else 'red'\n",
    "                plt.text(i, model_value * 0.95, f'{improvement:+.1f}%',\n",
    "                         ha='center', va='center', color=color, fontweight='bold')\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        plt.xticks(rotation=30, ha='right')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{viz_dir}/{metric}_comparison.png\")\n",
    "        plt.close()\n",
    "        print(f\"Saved {metric} comparison chart to {viz_dir}/{metric}_comparison.png\")\n",
    "\n",
    "# 2. Plot confusion matrices for all models\n",
    "def plot_all_confusion_matrices(base_model, loaded_models, validation_data, tokenizer):\n",
    "    \"\"\"Generate confusion matrices for base model and all loaded PEFT models\"\"\"\n",
    "    # Function to get predictions from a model\n",
    "    def get_predictions(model, dataset):\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=TrainingArguments(\n",
    "                output_dir=\"./tmp\",\n",
    "                per_device_eval_batch_size=32,\n",
    "                report_to=\"none\",\n",
    "            ),\n",
    "            eval_dataset=dataset,\n",
    "        )\n",
    "        outputs = trainer.predict(dataset)\n",
    "        return outputs.predictions.argmax(axis=1), outputs.label_ids\n",
    "    \n",
    "    # Plot base model confusion matrix if not already done\n",
    "    base_cm_path = f\"{viz_dir}/base_model_confusion_matrix.png\"\n",
    "    if not os.path.exists(base_cm_path):\n",
    "        pred_labels, true_labels = get_predictions(base_model, validation_data)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(true_labels, pred_labels)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "        plt.title('Confusion Matrix - Base Model')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        \n",
    "        if cm.shape[0] == 2:\n",
    "            plt.xticks([0.5, 1.5], ['Negative', 'Positive'])\n",
    "            plt.yticks([0.5, 1.5], ['Negative', 'Positive'])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(base_cm_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved base model confusion matrix to {base_cm_path}\")\n",
    "    \n",
    "    # Plot confusion matrices for all loaded models\n",
    "    for model_name, model in loaded_models.items():\n",
    "        if \"confusion_matrix\" in model_name.lower():\n",
    "            continue  # Skip if already has confusion in the name\n",
    "            \n",
    "        cm_path = f\"{viz_dir}/{model_name.replace(' ', '_').lower()}_confusion_matrix.png\"\n",
    "        if not os.path.exists(cm_path):\n",
    "            try:\n",
    "                pred_labels, true_labels = get_predictions(model, validation_data)\n",
    "                \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                cm = confusion_matrix(true_labels, pred_labels)\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "                plt.title(f'Confusion Matrix - {model_name}')\n",
    "                plt.ylabel('True Label')\n",
    "                plt.xlabel('Predicted Label')\n",
    "                \n",
    "                if cm.shape[0] == 2:\n",
    "                    plt.xticks([0.5, 1.5], ['Negative', 'Positive'])\n",
    "                    plt.yticks([0.5, 1.5], ['Negative', 'Positive'])\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(cm_path)\n",
    "                plt.close()\n",
    "                print(f\"Saved {model_name} confusion matrix to {cm_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating confusion matrix for {model_name}: {e}\")\n",
    "\n",
    "# 3. Parameter efficiency visualization\n",
    "def plot_parameter_efficiency():\n",
    "    \"\"\"Create parameter efficiency visualizations for various models\"\"\"\n",
    "    # Create pies showing trainable vs frozen parameters\n",
    "    for model_name, model_metrics in all_results.items():\n",
    "        if model_name == \"Base Model\" or \"trainable_pct\" not in model_metrics:\n",
    "            continue\n",
    "            \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Calculate frozen parameters\n",
    "        trainable_pct = model_metrics[\"trainable_pct\"]\n",
    "        frozen_pct = 100 - trainable_pct\n",
    "        \n",
    "        # Create pie chart\n",
    "        sizes = [trainable_pct, frozen_pct]\n",
    "        labels = ['Trainable Parameters', 'Frozen Parameters']\n",
    "        colors = ['#ff9999', '#66b3ff']\n",
    "        explode = (0.1, 0)  # Explode the trainable parameters\n",
    "        \n",
    "        plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "                autopct='%1.1f%%', shadow=True, startangle=90,\n",
    "                textprops={'fontsize': 14})\n",
    "        \n",
    "        plt.axis('equal')  # Equal aspect ratio ensures pie is drawn as a circle\n",
    "        \n",
    "        if \"trainable_params\" in model_metrics and \"total_params\" in model_metrics:\n",
    "            trainable = model_metrics[\"trainable_params\"]\n",
    "            total = model_metrics[\"total_params\"]\n",
    "            plt.title(f'{model_name} Parameter Efficiency\\n{trainable:,} of {total:,} parameters',\n",
    "                    fontsize=16)\n",
    "        else:\n",
    "            plt.title(f'{model_name} Parameter Efficiency', fontsize=16)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{viz_dir}/{model_name.replace(' ', '_').lower()}_parameter_efficiency.png\")\n",
    "        plt.close()\n",
    "        print(f\"Saved parameter efficiency for {model_name} to {viz_dir}/{model_name.replace(' ', '_').lower()}_parameter_efficiency.png\")\n",
    "    \n",
    "    # Create parameter efficiency vs performance plot\n",
    "    if any(\"trainable_pct\" in metrics for metrics in all_results.values() if metrics != all_results[\"Base Model\"]):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Extract data\n",
    "        model_names = []\n",
    "        param_efficiency = []\n",
    "        accuracy_values = []\n",
    "        \n",
    "        for model_name, metrics in all_results.items():\n",
    "            if model_name != \"Base Model\" and \"trainable_pct\" in metrics:\n",
    "                model_names.append(model_name)\n",
    "                param_efficiency.append(metrics[\"trainable_pct\"])\n",
    "                accuracy_values.append(metrics[\"eval_accuracy\"])\n",
    "        \n",
    "        # Create scatter plot\n",
    "        plt.scatter(param_efficiency, accuracy_values, s=100, c=range(len(model_names)), cmap='viridis')\n",
    "        \n",
    "        # Add labels for each point\n",
    "        for i, model in enumerate(model_names):\n",
    "            plt.annotate(model, (param_efficiency[i], accuracy_values[i]), \n",
    "                        fontsize=9, ha='right', va='bottom')\n",
    "        \n",
    "        # Add reference line for base model accuracy\n",
    "        base_accuracy = all_results[\"Base Model\"][\"eval_accuracy\"]\n",
    "        plt.axhline(y=base_accuracy, color='r', linestyle='--', alpha=0.7,\n",
    "                   label=f'Base Model Accuracy: {base_accuracy:.4f}')\n",
    "        \n",
    "        # Labels and title\n",
    "        plt.xlabel('Percentage of Parameters Trained (%)', fontsize=12)\n",
    "        plt.ylabel('Accuracy', fontsize=12)\n",
    "        plt.title('Accuracy vs Parameter Efficiency', fontsize=14)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{viz_dir}/efficiency_vs_accuracy.png\")\n",
    "        plt.close()\n",
    "        print(f\"Saved efficiency vs accuracy plot to {viz_dir}/efficiency_vs_accuracy.png\")\n",
    "\n",
    "# 4. Training history visualization \n",
    "def plot_training_curves():\n",
    "    \"\"\"Plot training curves from saved history if available\"\"\"\n",
    "    # Plot LoRA learning curves if history exists\n",
    "    lora_curves_path = f\"{viz_dir}/lora_learning_curves.png\"\n",
    "    if not os.path.exists(lora_curves_path) and 'peft_trainer' in locals() and hasattr(peft_trainer, 'state'):\n",
    "        history = peft_trainer.state.log_history\n",
    "        \n",
    "        # Extract training loss\n",
    "        train_loss = []\n",
    "        train_steps = []\n",
    "        eval_loss = []\n",
    "        eval_acc = []\n",
    "        eval_steps = []\n",
    "        \n",
    "        for entry in history:\n",
    "            if 'loss' in entry and 'eval_loss' not in entry:\n",
    "                train_loss.append(entry['loss'])\n",
    "                train_steps.append(entry['step'])\n",
    "            if 'eval_loss' in entry:\n",
    "                eval_loss.append(entry['eval_loss'])\n",
    "                eval_acc.append(entry['eval_accuracy'])\n",
    "                eval_steps.append(entry['step'])\n",
    "        \n",
    "        if train_loss and eval_acc:  # Only create plot if we have data\n",
    "            # Create the plot\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            \n",
    "            # Plot training loss\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(train_steps, train_loss, 'b-', marker='o', markersize=4, alpha=0.7)\n",
    "            plt.title('Training Loss', fontsize=14)\n",
    "            plt.xlabel('Steps', fontsize=12)\n",
    "            plt.ylabel('Loss', fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot evaluation accuracy\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(eval_steps, eval_acc, 'g-', marker='o', markersize=6, label='Accuracy')\n",
    "            \n",
    "            # If eval loss exists, plot it on secondary axis\n",
    "            if eval_loss:\n",
    "                ax2 = plt.gca().twinx()\n",
    "                ax2.plot(eval_steps, eval_loss, 'r--', marker='x', markersize=4, alpha=0.7, label='Loss')\n",
    "                ax2.set_ylabel('Loss', color='r', fontsize=12)\n",
    "                ax2.tick_params(axis='y', colors='r')\n",
    "            \n",
    "            plt.title('Validation Metrics', fontsize=14)\n",
    "            plt.xlabel('Steps', fontsize=12)\n",
    "            plt.ylabel('Accuracy', fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend(loc='lower right')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(lora_curves_path)\n",
    "            plt.close()\n",
    "            print(f\"Saved LoRA learning curves to {lora_curves_path}\")\n",
    "\n",
    "# 5. LoRA variants comparison if data exists\n",
    "def plot_lora_variants_comparison():\n",
    "    \"\"\"Plot comparisons of LoRA variants if data exists\"\"\"\n",
    "    # Check if comparison data exists\n",
    "    if 'comparison_results' in locals() and comparison_results:\n",
    "        variants_path = f\"{viz_dir}/lora_variants_comparison.png\"\n",
    "        if not os.path.exists(variants_path):\n",
    "            config_df = pd.DataFrame(comparison_results)\n",
    "            \n",
    "            plt.figure(figsize=(15, 12))\n",
    "            \n",
    "            # Plot accuracy comparison\n",
    "            plt.subplot(2, 2, 1)\n",
    "            sns.barplot(x='config_name', y='accuracy', data=config_df, palette='viridis')\n",
    "            plt.title('Accuracy Comparison', fontsize=14)\n",
    "            plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "            plt.ylabel('Accuracy', fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot F1 score comparison\n",
    "            plt.subplot(2, 2, 2)\n",
    "            sns.barplot(x='config_name', y='f1', data=config_df, palette='viridis')\n",
    "            plt.title('F1 Score Comparison', fontsize=14)\n",
    "            plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "            plt.ylabel('F1 Score', fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot training time comparison\n",
    "            plt.subplot(2, 2, 3)\n",
    "            sns.barplot(x='config_name', y='training_time_min', data=config_df, palette='viridis')\n",
    "            plt.title('Training Time Comparison', fontsize=14)\n",
    "            plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "            plt.ylabel('Training Time (min)', fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot parameter efficiency comparison\n",
    "            plt.subplot(2, 2, 4)\n",
    "            sns.barplot(x='config_name', y='param_efficiency', data=config_df, palette='viridis')\n",
    "            plt.title('Parameter Efficiency (% of Total)', fontsize=14)\n",
    "            plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "            plt.ylabel('Parameter Efficiency (%)', fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(variants_path)\n",
    "            plt.close()\n",
    "            print(f\"Saved LoRA variants comparison to {variants_path}\")\n",
    "            \n",
    "            # Create radar chart\n",
    "            radar_path = f\"{viz_dir}/lora_variants_radar.png\"\n",
    "            if not os.path.exists(radar_path):\n",
    "                try:\n",
    "                    from math import pi\n",
    "                    \n",
    "                    # Prepare data for radar chart\n",
    "                    categories = ['Accuracy', 'F1 Score', 'Parameter Efficiency', 'Speed']\n",
    "                    \n",
    "                    # Normalize values for radar chart\n",
    "                    max_acc = config_df['accuracy'].max()\n",
    "                    max_f1 = config_df['f1'].max()\n",
    "                    min_params = config_df['param_efficiency'].min()\n",
    "                    max_params = config_df['param_efficiency'].max()\n",
    "                    min_time = config_df['training_time_sec'].min()\n",
    "                    max_time = config_df['training_time_sec'].max()\n",
    "                    \n",
    "                    # Create radar values (normalized from 0 to 1)\n",
    "                    radar_data = {}\n",
    "                    for _, row in config_df.iterrows():\n",
    "                        # Normalize values to 0-1 scale\n",
    "                        acc_norm = row['accuracy'] / max_acc if max_acc else 0\n",
    "                        f1_norm = row['f1'] / max_f1 if max_f1 else 0\n",
    "                        \n",
    "                        # For params, higher efficiency is better\n",
    "                        if max_params == min_params:\n",
    "                            param_norm = 1.0\n",
    "                        else:\n",
    "                            param_norm = (row['param_efficiency'] - min_params) / (max_params - min_params)\n",
    "                        \n",
    "                        # For time, faster is better (invert)\n",
    "                        if max_time == min_time:\n",
    "                            time_norm = 1.0\n",
    "                        else:\n",
    "                            time_norm = 1 - ((row['training_time_sec'] - min_time) / (max_time - min_time))\n",
    "                        \n",
    "                        radar_data[row['config_name']] = [acc_norm, f1_norm, param_norm, time_norm]\n",
    "                    \n",
    "                    # Number of variables\n",
    "                    N = len(categories)\n",
    "                    \n",
    "                    # Compute angle for each axis\n",
    "                    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "                    angles += angles[:1]  # Close the loop\n",
    "                    \n",
    "                    # Initialize the plot\n",
    "                    plt.figure(figsize=(10, 10))\n",
    "                    ax = plt.subplot(111, polar=True)\n",
    "                    \n",
    "                    # Draw one axis per variable and add labels\n",
    "                    plt.xticks(angles[:-1], categories, fontsize=12)\n",
    "                    \n",
    "                    # Draw the chart for each config\n",
    "                    for i, (config_name, values) in enumerate(radar_data.items()):\n",
    "                        values += values[:1]  # Close the loop\n",
    "                        ax.plot(angles, values, linewidth=2, linestyle='solid', label=config_name)\n",
    "                        ax.fill(angles, values, alpha=0.1)\n",
    "                    \n",
    "                    # Add legend\n",
    "                    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "                    plt.title('LoRA Configuration Comparison', fontsize=15)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(radar_path)\n",
    "                    plt.close()\n",
    "                    print(f\"Saved radar chart comparison to {radar_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating radar chart: {e}\")\n",
    "\n",
    "# 6. Memory usage comparison \n",
    "def plot_memory_comparison():\n",
    "    \"\"\"Plot memory usage comparison if memory data exists\"\"\"\n",
    "    memory_data = {}\n",
    "    \n",
    "    # Extract memory usage from base_memory and other variables if they exist\n",
    "    if 'base_memory' in locals():\n",
    "        memory_data[\"Base Model\"] = base_memory[\"allocated_mb\"]\n",
    "    \n",
    "    if 'lora_memory' in locals():\n",
    "        memory_data[\"LoRA Model\"] = lora_memory[\"allocated_mb\"]\n",
    "        \n",
    "    if 'qlora_memory' in locals():\n",
    "        memory_data[\"QLoRA Model\"] = qlora_memory[\"allocated_mb\"]\n",
    "    \n",
    "    if memory_data and len(memory_data) > 1:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Create bar chart\n",
    "        models = list(memory_data.keys())\n",
    "        memory_values = list(memory_data.values())\n",
    "        \n",
    "        bars = plt.bar(models, memory_values, color=sns.color_palette(\"viridis\", len(models)))\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, height + 1,\n",
    "                    f'{height:.1f} MB',\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.ylabel('Memory Usage (MB)', fontsize=12)\n",
    "        plt.title('Memory Usage Comparison', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add reduction percentages\n",
    "        if \"Base Model\" in memory_data:\n",
    "            base_mem = memory_data[\"Base Model\"]\n",
    "            for i, (model, mem) in enumerate(zip(models, memory_values)):\n",
    "                if model != \"Base Model\":\n",
    "                    reduction = (base_mem - mem) / base_mem * 100\n",
    "                    plt.text(i, mem / 2,\n",
    "                            f'{reduction:+.1f}%',\n",
    "                            ha='center', va='center',\n",
    "                            color='white', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{viz_dir}/memory_usage_comparison.png\")\n",
    "        plt.close()\n",
    "        print(f\"Saved memory usage comparison to {viz_dir}/memory_usage_comparison.png\")\n",
    "\n",
    "# 7. Final comprehensive comparison visualization\n",
    "def create_final_comparison():\n",
    "    \"\"\"Create a comprehensive summary visualization of all models\"\"\"\n",
    "    # Create a table-like visualization comparing all models\n",
    "    if all_results:\n",
    "        # Determine which metrics are available\n",
    "        metrics = []\n",
    "        for metric_name in ['accuracy', 'f1', 'precision', 'recall']:\n",
    "            if f'eval_{metric_name}' in all_results[\"Base Model\"]:\n",
    "                metrics.append(metric_name)\n",
    "        \n",
    "        if not metrics:\n",
    "            return\n",
    "            \n",
    "        # Extract data\n",
    "        models = list(all_results.keys())\n",
    "        data = []\n",
    "        \n",
    "        for model in models:\n",
    "            row = [model]\n",
    "            for metric in metrics:\n",
    "                if f'eval_{metric}' in all_results[model]:\n",
    "                    row.append(all_results[model][f'eval_{metric}'])\n",
    "                else:\n",
    "                    row.append(None)\n",
    "            data.append(row)\n",
    "        \n",
    "        # Convert to DataFrame for easier handling\n",
    "        df = pd.DataFrame(data, columns=['Model'] + [m.capitalize() for m in metrics])\n",
    "        \n",
    "        # Create a visually appealing table\n",
    "        fig, ax = plt.subplots(figsize=(12, len(models) * 0.8 + 2))\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Calculate improvements relative to base model\n",
    "        improvements = {}\n",
    "        base_idx = df.index[df['Model'] == 'Base Model'].tolist()[0]\n",
    "        base_values = df.iloc[base_idx, 1:].values\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            if row['Model'] != 'Base Model':\n",
    "                improvements[i] = []\n",
    "                for j, val in enumerate(row.values[1:]):\n",
    "                    if val is not None and base_values[j] is not None:\n",
    "                        pct = (val - base_values[j]) / base_values[j] * 100\n",
    "                        improvements[i].append(f\"{val:.4f}\\n({pct:+.1f}%)\")\n",
    "                    else:\n",
    "                        improvements[i].append(\"\")\n",
    "        \n",
    "        # Create the table\n",
    "        table_data = []\n",
    "        for i, row in df.iterrows():\n",
    "            if row['Model'] == 'Base Model':\n",
    "                table_data.append([row['Model']] + [f\"{v:.4f}\" for v in row.values[1:]])\n",
    "            else:\n",
    "                table_data.append([row['Model']] + improvements[i])\n",
    "        \n",
    "        table = ax.table(cellText=table_data, colLabels=df.columns, loc='center',\n",
    "                         cellLoc='center', colColours=['#f2f2f2']*len(df.columns))\n",
    "        \n",
    "        # Customize table appearance\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1, 1.5)  # Adjust row height\n",
    "        \n",
    "        # Highlight the best value in each column\n",
    "        for j in range(1, len(df.columns)):\n",
    "            col_values = df.iloc[:, j].values\n",
    "            best_idx = np.nanargmax(col_values)\n",
    "            if best_idx != base_idx:  # If best is not base model\n",
    "                cell = table[best_idx+1, j]\n",
    "                cell.set_facecolor('#d4f7d4')  # Light green\n",
    "        \n",
    "        plt.title('Comprehensive Model Comparison', fontsize=16, pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{viz_dir}/comprehensive_comparison.png\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved comprehensive comparison to {viz_dir}/comprehensive_comparison.png\")\n",
    "\n",
    "# 8. Create sample prediction visualization if data exists\n",
    "def plot_sample_predictions():\n",
    "    \"\"\"Create visualization of sample predictions if data exists\"\"\"\n",
    "    sample_pred_path = f\"{results_dir}/sample_predictions.csv\"\n",
    "    if os.path.exists(sample_pred_path):\n",
    "        try:\n",
    "            sample_df = pd.read_csv(sample_pred_path)\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Create shortened texts for display\n",
    "            texts_short = [t[:30] + \"...\" if len(t) > 30 else t for t in sample_df['text']]\n",
    "            \n",
    "            # Sort by confidence for better visualization\n",
    "            sample_df['prediction_numeric'] = sample_df['prediction'].apply(lambda x: 1 if x == 1 or x == \"Positive\" else 0)\n",
    "            sample_df = sample_df.sort_values(['prediction_numeric', 'confidence'], ascending=[False, False])\n",
    "            \n",
    "            # Get sorted data\n",
    "            texts_short = [texts_short[i] for i in sample_df.index]\n",
    "            predictions = sample_df['prediction_numeric'].values\n",
    "            confidences = sample_df['confidence'].values\n",
    "            \n",
    "            # Create colored confidence bars\n",
    "            colors = ['green' if p == 1 else 'red' for p in predictions]\n",
    "            \n",
    "            # Plot bars\n",
    "            plt.figure(figsize=(14, 7))\n",
    "            bars = plt.bar(range(len(texts_short)), confidences, color=colors)\n",
    "            \n",
    "            # Add text labels\n",
    "            for i, bar in enumerate(bars):\n",
    "                plt.text(i, bar.get_height() + 0.02, texts_short[i],\n",
    "                        rotation=45, ha='right', fontsize=9)\n",
    "            \n",
    "            # Customize plot\n",
    "            plt.ylim(0, 1.1)  # Leave room for text labels\n",
    "            plt.xticks([])  # Hide x-axis labels as we've added text annotations\n",
    "            plt.ylabel('Prediction Confidence', fontsize=12)\n",
    "            plt.title('Sentiment Analysis Prediction Confidence', fontsize=14)\n",
    "            \n",
    "            # Add legend\n",
    "            from matplotlib.patches import Patch\n",
    "            legend_elements = [\n",
    "                Patch(facecolor='green', label='Positive'),\n",
    "                Patch(facecolor='red', label='Negative')\n",
    "            ]\n",
    "            plt.legend(handles=legend_elements)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{viz_dir}/sample_predictions_confidence.png\")\n",
    "            plt.close()\n",
    "            print(f\"Saved sample predictions visualization to {viz_dir}/sample_predictions_confidence.png\")\n",
    "            \n",
    "            # Create stacked bar chart of positive/negative probabilities\n",
    "            if 'pos_prob' in sample_df.columns and 'neg_prob' in sample_df.columns:\n",
    "                plt.figure(figsize=(14, 7))\n",
    "                \n",
    "                indices = np.arange(len(texts_short))\n",
    "                width = 0.35\n",
    "                \n",
    "                # Plot positive and negative probabilities\n",
    "                plt.bar(indices, sample_df['pos_prob'], width, label='Positive', color='green', alpha=0.7)\n",
    "                plt.bar(indices + width, sample_df['neg_prob'], width, label='Negative', color='red', alpha=0.7)\n",
    "                \n",
    "                # Add text labels\n",
    "                for i in range(len(texts_short)):\n",
    "                    plt.text(i + width/2, -0.05, texts_short[i], rotation=45, ha='right', fontsize=9)\n",
    "                \n",
    "                # Customize plot\n",
    "                plt.xlabel('Examples')\n",
    "                plt.ylabel('Probability')\n",
    "                plt.title('Positive and Negative Sentiment Probabilities')\n",
    "                plt.legend()\n",
    "                plt.xticks([])\n",
    "                plt.grid(axis='y', alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.subplots_adjust(bottom=0.15)  # Make room for text labels\n",
    "                plt.savefig(f\"{viz_dir}/sample_predictions_probabilities.png\")\n",
    "                plt.close()\n",
    "                print(f\"Saved probability distribution visualization to {viz_dir}/sample_predictions_probabilities.png\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating sample predictions visualization: {e}\")\n",
    "\n",
    "# Execute all visualization functions\n",
    "try:\n",
    "    # 1. Model performance comparison\n",
    "    print(\"\\nGenerating model performance comparisons...\")\n",
    "    plot_model_comparison(all_results)\n",
    "    \n",
    "    # 2. Parameter efficiency visualization\n",
    "    print(\"\\nGenerating parameter efficiency visualizations...\")\n",
    "    plot_parameter_efficiency()\n",
    "    \n",
    "    # 3. Training curves visualization\n",
    "    print(\"\\nGenerating training curves...\")\n",
    "    plot_training_curves()\n",
    "    \n",
    "    # 4. LoRA variants comparison\n",
    "    print(\"\\nGenerating LoRA configuration comparisons...\")\n",
    "    plot_lora_variants_comparison()\n",
    "    \n",
    "    # 5. Memory usage comparison\n",
    "    print(\"\\nGenerating memory usage comparison...\")\n",
    "    plot_memory_comparison()\n",
    "    \n",
    "    # 6. Final comprehensive comparison\n",
    "    print(\"\\nGenerating comprehensive comparison...\")\n",
    "    create_final_comparison()\n",
    "    \n",
    "    # 7. Sample predictions visualization\n",
    "    print(\"\\nGenerating sample predictions visualization...\")\n",
    "    plot_sample_predictions()\n",
    "    \n",
    "    # 8. Confusion matrices for all models if not already created\n",
    "    if 'base_model' in globals() and 'loaded_models' in globals() and loaded_models and 'encoded_validation' in globals():\n",
    "        print(\"\\nGenerating confusion matrices for all models...\")\n",
    "        plot_all_confusion_matrices(base_model, loaded_models, encoded_validation, tokenizer)\n",
    "    \n",
    "    print(\"\\nAll visualizations complete! Check the output directory:\", viz_dir)\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error during visualization generation: {e}\")\n",
    "    traceback.print_exc()\n",
    "    print(\"Continuing with available visualizations...\")\n",
    "\n",
    "# Print summary of created visualizations\n",
    "try:\n",
    "    viz_files = os.listdir(viz_dir)\n",
    "    print(f\"\\nCreated {len(viz_files)} visualization files:\")\n",
    "    for i, file in enumerate(sorted(viz_files)):\n",
    "        print(f\"{i+1}. {file}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ba4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
